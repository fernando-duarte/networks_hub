import Pkg
Pkg.add("Convex")
Pkg.add("SCS") #  make OPT="-O3 -march=native" DLONG=1 USE_OPENMP=1 BLASLDFLAGS="-L$JULIA_HOME/lib/julia -lopenblas64_" BLAS64=1 BLASSUFFIX=_64_ LD_LIBRARY_PATH="$JULIA_HOME/lib/julia"

Pkg.rm("SCS")
Pkg.add(Pkg.PackageSpec(url="https://github.com/jump-dev/SCS.jl", rev="master")) # From url to remote gitrepo

Pkg.add(Pkg.PackageSpec(url="https://github.com/rdoelman/SequentialConvexRelaxation.jl"))
#Pkg.add(Pkg.PackageSpec(name="Convex", version="0.13.1"))
Pkg.add(Pkg.PackageSpec(url="https://github.com/jump-dev/Convex.jl", rev="master")) # From url to remote gitrepo

Pkg.add(Pkg.PackageSpec(name="Convex"))
Pkg.update("Convex")
Pkg.add("QDLDL")

# changed at /home/ec2-user/.julia/packages/SequentialConvexRelaxation/9jc2O/src/SequentialConvexRelaxation.jl:93
# from
# G = inv(E'*E + weight_update_tuning_param * Matrix{Float64}(I,size(C,2),size(C,2)))
# to
# G = inv(E'*E .+ weight_update_tuning_param * Matrix{Float64}(I,size(C,2),size(C,2)))
# to not get error when E is Float64

using Convex, SCS, SequentialConvexRelaxation, QDLDL
using LinearAlgebra, Test

using LinearAlgebra, DataFrames, XLSX, Missings

## load data
xf = XLSX.readxlsx("node_stats_forsimulation_all.xlsx") 
data = vcat( [(XLSX.eachtablerow(xf[s]) |> DataFrames.DataFrame) for s in XLSX.sheetnames(xf)]... )
unique!(data) # delete duplicate rows, use `nonunique(data)' to see if there are any duplicates
data = data[isequal.(data.qt_dt,192), :] # keep quarter == 195 = 2008q4
sort!(data, :assets, rev = true)
data = data[1:5,:] # keep small number of nodes, for testing
N = size(data,1) # number of nodes
units = 1e6;
data[:,[:w, :c, :assets, :p_bar, :b]] .= data[!,[:w, :c, :assets, :p_bar, :b]]./units
data.b[:] .= missing
data.c[:] .= missing

col_with_miss = names(data)[[any(ismissing.(col)) for col = eachcol(data)]] # columns with at least one missing
data_nm = coalesce.(data, data.w .+ 0.01) # replace missing by a value
nm_c = findall(x->x==0,ismissing.(data.c))
nm_b = findall(x->x==0,ismissing.(data.b))
dropmissing(data, [:delta, :delta_alt, :w, :assets, :p_bar]) # remove type missing

names(data) # column names
describe(data)
show(data, true)

p_bar = data_nm.p_bar
assets = data_nm.assets

## fixed point
# x0 = [1;1]
# function contraction(p,x)
#    min.(p_bar, max.(A'*p .+ c .- x.*c,0))
# end
# contraction_iter(x, n::Integer) = n <= 0 ? p_bar  : contraction(contraction_iter(x,n-1),x)

# aux = Variable(1,1)
# theta = 1.1
# constraint2 =(theta*aux>=A[1,2])
# constraint3 = (theta*(1-aux)>=A[2,1])

#p = Variable(2,Positive())
g = 0
x = rand(N,1)
N = length(p_bar)
A = Variable(N,N,Positive())
b = Variable(N,Positive())
c = Variable(N,Positive())
#p = Variable(N,Positive())
upperA = (A <= 1)
upperb = (b <= p_bar)
upperc = (c <= assets)
#upperp = (p .<= p_bar)
ac = (A'*p_bar + c - assets == 0)
lc = (p_bar'*A*ones(N,1) + b - p_bar == 0)
expr = (1+g)*(A'*p_bar+c-x.*c)-g*p_bar

#fix!(x, v),free!(x)

problem = maximize(sum(min(p_bar,expr)))
problem.constraints = [upperA, upperb, upperc, ac, lc]
# Convex.solve!(problem,  SCS.Optimizer(verbose=true), warmstart=false)
# evaluate(A)
# #p.constraints[1].dual
# Asol = evaluate(A)
# tol = 1e-6
# @testset "convex_problem" begin
#     @test all(Asol.-1 .<= tol)
#     @test all(Asol .>= -tol)
#     @test theta*auxsol-Asol[1,2]>=-tol
#     @test theta*(1-auxsol)-Asol[2,1]>=-tol
# end


H = zeros(N^2,N^2);
for i=1:N
    for j=1:N
        temp = zeros(N,N)
        if i==j
            temp[j,i]=2.0 
        else
            temp[j,i]=1.0 
        end
        H[(i-1)*N+1:i*N,(j-1)*N+1:j*N] = temp
    end
end


constraint8 = BilinearConstraint(vec(A)',H,vec(A),0.0,X=-zeros(1,N^2)/2,Y=-zeros(N^2,1)/2,λ=15.0, W1=Matrix(I,1,1)/2, W2=Matrix(I,1,1)/2)
bp=BilinearProblem(problem,constraint8)
SequentialConvexRelaxation.solve!(bp,()->SCS.Optimizer(verbose=1,acceleration_lookback=1,max_iters=100000,eps=1e-8),iterations=10,update_weights=true,weight_update_tuning_param=0.1)
#constraint9 = BinaryConstraint(aux,X=0.4,λ = 2.0)

SequentialConvexRelaxation.solve!(bp,()->SCS.Optimizer,iterations=10,update_weights=true,weight_update_tuning_param=0.1)


Asol = evaluate(A)
vec(Asol)'*H*vec(Asol)
tol = 1e-6
@testset "convex_problem" begin
    @test all(Asol.-1 .<= tol)
    @test all(Asol .>= -tol)
    @test theta*auxsol-Asol[1,2]>=-tol
    @test theta*(1-auxsol)-Asol[2,1]>=-tol
end


Aplot = copy(Asol)
Aplot[Aplot.<1f-4].=0
Aplot


julia> Pkg.clone("https://github.com/rdeits/CouenneNL.jl")
julia> Pkg.build("CouenneNL")
PiecewiseLinearOpt.jl




import Pkg
Pkg.add("Pajarito")
Pkg.add("GLPKMathProgInterface")
Pkg.add("Convex")
using Pajarito, GLPKMathProgInterface, SCS #load packages
using GLPK
mysolver = PajaritoSolver(log_level = 3, #use verbose output
mip_solver = GLPKSolverMIP(msg_lev = GLPK.MSG_OFF), #set MIP solver
cont_solver = SCSSolver(eps = 1e-6, verbose = 0)) #set conic solver


using Convex, LinearAlgebra
p=1;m=2;n=3;V=rand(n,p)
mp = Variable(p, Positive(), :Int) #create p nonneg. integer variables
eOpt = maximize(eigmin(V * diag(mp./m) * V'), #max. min. eigenvalue
sum(mp) <= m) #add linear constraint
solve!(eOpt, Pajarito.mysolver()) #solve model using Pajarito solver
@show eOpt.status, eOpt.optval, mp.value #show solve status and results



Pkg.add("Pavito")
using Convex, Pavito

# Set up Convex.jl model, solve, print solution
function gatesizing(yUB, solver)
    fe = [1, 0.8, 1, 0.7, 0.7, 0.5, 0.5] .* [1, 2, 1, 1.5, 1.5, 1, 2]
    Cout6 = 10
    Cout7 = 10

    y = Variable(7, Positive())
    z = Variable(yUB, 7, Positive(), :Bin)

    D1 = exp(-y[1]) + exp(-y[1] + y[4])
    D2 = 2 * exp(-y[2]) + exp(-y[2] + y[4]) + exp(-y[2] + y[5])
    D3 = 2 * exp(-y[3]) + exp(-y[3] + y[5]) + exp(-y[3] + y[7])
    D4 = 2 * exp(-y[4]) + exp(-y[4] + y[6]) + exp(-y[4] + y[7])
    D5 = exp(-y[5]) + exp(-y[5] + y[7])
    D6 = Cout6 * exp(-y[6])
    D7 = Cout7 * exp(-y[7])

    P = minimize(
        maximum([(D1+D4+D6), (D1+D4+D7), (D2+D4+D6), (D2+D4+D7), (D2+D5+D7), (D3+D5+D6), (D3+D7)]),
        sum(fe .* exp(y)) <= 20,
        sum(exp(y)) <= 100)

    for i in 1:7
        P.constraints += (sum(z[:,i]) == 1)
        P.constraints += (y[i] == sum([log(j) * z[j,i] for j=1:yUB]))
    end

    solve!(P, solver)

    println("\nCircuit delay (obj) = $(P.optval)")
    println("Scale factors (exp(y)):\n$(exp.(y.value))")
    println("Value indicators (z):\n$(round.(z.value))")
end


#=========================================================
Choose solvers and options
=========================================================#

mip_solver_drives = false
rel_gap = 1e-5


using Cbc
mip_solver = CbcSolver()

using CPLEX
mip_solver = CplexSolver(
    CPX_PARAM_SCRIND=(mip_solver_drives ? 1 : 0),
    CPX_PARAM_EPINT=1e-8,
    CPX_PARAM_EPRHS=1e-7,
    CPX_PARAM_EPGAP=(mip_solver_drives ? 1e-5 : 1e-9)
)


using Ipopt
cont_solver = IpoptSolver(print_level=0)

solver = PavitoSolver(
    mip_solver_drives=mip_solver_drives,
    log_level=2,
    rel_gap=rel_gap,
	mip_solver=mip_solver,
	cont_solver=cont_solver,
)


#=========================================================
Specify data
=========================================================#

# Integer upper bound on integer gate size factors
# Optimal solution for this instances has all exp(y) <= 3,
# so yUB larger than 2 is not constraining, but impacts number of variables
yUB = 3


#=========================================================
Solve Convex.jl model
=========================================================#

# Optimal solution is [2,3,3,3,2,3,3] with value 8.3333
gatesizing(yUB, solver)





function lasso_admm(A, b, lam, x; tol=1e-8, maxit=50000)
  u = zero(x)
  z = copy(x)
  f = LeastSquares(A, b)
  g = NormL1(lam)
  gam = 100.0/norm(A)^2
  for it = 1:maxit
    # perform f-update step
    prox!(x, f, z - u, gam)
    # perform g-update step
    prox!(z, g, x + u, gam)
    # stopping criterion
    if norm(x-z, Inf) <= tol*(1+norm(u, Inf))
      break
    end
    # dual update
    u .+= x - z
  end
  return z
end


import Pkg
Pkg.add("ProximalAlgorithms")
Pkg.add("ProximalOperators")
using ProximalAlgorithms
using ProximalOperators
using LinearAlgebra
using Random
using Test

@testset "Nonconvex QP (tiny, $T)" for T in [Float64]
    Q = Matrix(Diagonal(T[-0.5, 1.0]))
    q = T[0.3, 0.5]
    low = T(-1.0)
    upp = T(+1.0)

    f = Quadratic(Q, q)
    g = IndBox(low, upp)

    n = 2

    Lip = maximum(diag(Q))
    gamma = T(0.95) / Lip

    @testset "PANOC" begin
        x0 = zeros(T, n)
        solver = ProximalAlgorithms.PANOC{T}()
        x, it = solver(x0, f = f, g = g)
        z = min.(upp, max.(low, x .- gamma .* (Q * x + q)))
        @test norm(x - z, Inf) / gamma <= solver.tol
    end

    @testset "ZeroFPR" begin
        x0 = zeros(T, n)
        solver = ProximalAlgorithms.ZeroFPR{T}()
        x, it = solver(x0, f = f, g = g)
        z = min.(upp, max.(low, x .- gamma .* (Q * x + q)))
        @test norm(x - z, Inf) / gamma <= solver.tol
    end

    @testset "LiLin" begin
        x0 = zeros(T, n)
        solver = ProximalAlgorithms.LiLin{T}(gamma = gamma)
        x, it = solver(x0, f = f, g = g)
        z = min.(upp, max.(low, x .- gamma .* (Q * x + q)))
        @test norm(x - z, Inf) / gamma <= solver.tol
    end
end

@testset "Nonconvex QP (small, $T)" for T in [Float64]
    @testset "Random problem $k" for k = 1:5
        Random.seed!(k)

        n = 100
        A = randn(T, n, n)
        U, R = qr(A)
        eigenvalues = T(2) .* rand(T, n) .- T(1)
        Q = U * Diagonal(eigenvalues) * U'
        Q = 0.5 * (Q + Q')
        q = randn(T, n)

        low = T(-1.0)
        upp = T(+1.0)

        f = Quadratic(Q, q)
        g = IndBox(low, upp)

        Lip = maximum(abs.(eigenvalues))
        gamma = T(0.95) / Lip

        TOL = 1e-4

        @testset "PANOC" begin
            x0 = zeros(T, n)
            solver = ProximalAlgorithms.PANOC{T}(tol = TOL)
            x, it = solver(x0, f = f, g = g)
            z = min.(upp, max.(low, x .- gamma .* (Q * x + q)))
            @test norm(x - z, Inf) / gamma <= solver.tol
        end

        @testset "ZeroFPR" begin
            x0 = zeros(T, n)
            solver = ProximalAlgorithms.ZeroFPR{T}(tol = TOL)
            x, it = solver(x0, f = f, g = g)
            z = min.(upp, max.(low, x .- gamma .* (Q * x + q)))
            @test norm(x - z, Inf) / gamma <= solver.tol
        end

        @testset "LiLin" begin
            x0 = zeros(T, n)
            solver = ProximalAlgorithms.LiLin(gamma = gamma, tol = TOL)
            x, it = solver(x0, f = f, g = g)
            z = min.(upp, max.(low, x .- gamma .* (Q * x + q)))
            @test norm(x - z, Inf) / gamma <= solver.tol
        end
    end
end


indA = 1:N^2
indb = N^2+1:N^2+N
indc = N^2+N+1:N^2+2*N
indp = N^2+2*N+1:N^2+3*N
M = indc[end]

low = zeros(M)
upp = vcat(ones(indA[end]),p_bar,assets,p_bar)
g1 = IndBox(low, upp)
        

f = Quadratic(Q, q)





aux1 = hcat(repeat(diagm(p_bar),outer=[1,N]),Matrix{Float64}(I,N,N))
aux2 = hcat(repeat(diagm(p_bar),inner=[1,N]),zeros(N,N),Matrix{Float64}(I,N,N))

cons_liab = COSMO.Constraint(aux1, -p_bar, COSMO.ZeroSet,M,indA[1]:indb[end])
cons_assets = COSMO.Constraint(aux2, -assets, COSMO.ZeroSet,M,indA[1]:indc[end])

aux3 = zeros(2,M)
aux3[1,end-1]=1
aux3[2,end]=1
cons_q = COSMO.Constraint(aux3, zeros(2), COSMO.SecondOrderCone(2))

aux4 = zeros(2,M)
aux4[1,4]=1;aux4[1,end-1]=-1/sqrt(2);aux4[1,end]=-1/sqrt(2)
aux4[1,2]=1;aux4[1,end-1]=-1/sqrt(2);aux4[1,end]=1/sqrt(2)
cons_q2 = COSMO.Constraint(aux4, zeros(2), COSMO.ZeroSet)

constraints = [lb; ub; cons_liab; cons_assets; cons_q; cons_q2]

Linear(A,b)