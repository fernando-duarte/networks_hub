{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "039ba2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/SageMaker/networks_hub/SAN/code/network_simulations/joint_timing/Project.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg\n",
    "Pkg.activate(\"joint_timing\")\n",
    "Pkg.instantiate()\n",
    "\n",
    "using Cuba, Distributions\n",
    "using BenchmarkTools, Test, CUDA\n",
    "using FLoops, FoldsCUDA\n",
    "using SpecialFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44920bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int_thread_el (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M=25 # number of independent beta random variables\n",
    "atol=1e-6\n",
    "rtol=1e-3\n",
    "\n",
    "# integrate the pdf of the joint distribution -- should always equal 1\n",
    "function int(x, f)\n",
    "   f[1] = pdf(Product(Beta.(1.0,2.0*ones(M))),x)\n",
    "end\n",
    "\n",
    "# multithread\n",
    "function int_thread_col(x, f)\n",
    "    Threads.@threads for i in 1:size(x,2)\n",
    "      f[i] = pdf(Product(Beta.(1.0,2.0*ones(M))),@view(x[:,i]))\n",
    "    end\n",
    "end\n",
    "\n",
    "# multithread and loop to create product distribution\n",
    "function int_thread_el(x,f)\n",
    "   f[1,:] .= 1.0\n",
    "   Threads.@threads for j in 1:size(x,2)\n",
    "       for i in 1:size(x, 1)\n",
    "           f[1, j] *= pdf(Beta(1.0,2.0),@view(x[i,j]))\n",
    "       end\n",
    "   end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "469f42c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 1.6\n",
       " 1.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 1.6\n",
       " 1.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function pdf_beta(x::Array,alpha::T,beta::T) where T\n",
    "    x.^(alpha .- 1.0).*(1.0 .- x).^(beta .- 1.0)./SpecialFunctions.beta(1.0,2.0)\n",
    "end\n",
    "\n",
    "function pdf_beta(f,x,alpha,beta)\n",
    "    f=x.^(alpha-1.0).*(1.0 .- x).^(beta-1.0)./SpecialFunctions.beta(1.0,2.0)\n",
    "end\n",
    "\n",
    "display(pdf_beta(0.2,1.0,2.0))\n",
    "display(pdf_beta(0.0,0.2,1.0,2.0))\n",
    "\n",
    "display(pdf_beta([0.2,0.3],1.0,2.0))\n",
    "display(pdf_beta([0.0,0.0],[0.2,0.3],1.0,2.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d18af36",
   "metadata": {},
   "outputs": [],
   "source": [
    "function pdf_beta(x,a,b,dim)\n",
    "    prod(x.^(a-1.0f0) .* (1.0f0 .- x).^(b-1.0f0)./(gamma(a)*gamma(b)/gamma(a+b)),dims=dim)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8cf64318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2399999999999998"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf(Product(Beta.(1.0,2.0*ones(2))),[0.2,0.3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f62934b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(result, err) = suave(int, M, 1, atol = atol, rtol = rtol, maxevals = maxevals, nnew = nnew, nmin = nmin, flatness = flatness) = Component:\n",
      " 1: 0.9964268414111838 ± 0.0009925145844409677 (prob.: 1.0)\n",
      "Integrand evaluations: 5360000\n",
      "Number of subregions:  67\n",
      "Note: The desired accuracy was reached\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "MethodError: \u001b[0mCannot `convert` an object of type \u001b[92mVector{Float64}\u001b[39m\u001b[0m to an object of type \u001b[91mFloat64\u001b[39m\n\u001b[0mClosest candidates are:\n\u001b[0m  convert(::Type{T}, \u001b[91m::Union{InitialValues.SpecificInitialValue{typeof(*)}, InitialValues.SpecificInitialValue{typeof(Base.mul_prod)}}\u001b[39m) where T<:Union{AbstractString, Number} at /home/ec2-user/.julia/packages/InitialValues/EPz1F/src/InitialValues.jl:257\n\u001b[0m  convert(::Type{T}, \u001b[91m::LLVM.GenericValue\u001b[39m, \u001b[91m::LLVM.LLVMType\u001b[39m) where T<:AbstractFloat at /home/ec2-user/.julia/packages/LLVM/7Q46C/src/execution.jl:39\n\u001b[0m  convert(::Type{T}, \u001b[91m::LLVM.ConstantFP\u001b[39m) where T<:AbstractFloat at /home/ec2-user/.julia/packages/LLVM/7Q46C/src/core/value/constant.jl:98\n\u001b[0m  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: \u001b[0mCannot `convert` an object of type \u001b[92mVector{Float64}\u001b[39m\u001b[0m to an object of type \u001b[91mFloat64\u001b[39m\n\u001b[0mClosest candidates are:\n\u001b[0m  convert(::Type{T}, \u001b[91m::Union{InitialValues.SpecificInitialValue{typeof(*)}, InitialValues.SpecificInitialValue{typeof(Base.mul_prod)}}\u001b[39m) where T<:Union{AbstractString, Number} at /home/ec2-user/.julia/packages/InitialValues/EPz1F/src/InitialValues.jl:257\n\u001b[0m  convert(::Type{T}, \u001b[91m::LLVM.GenericValue\u001b[39m, \u001b[91m::LLVM.LLVMType\u001b[39m) where T<:AbstractFloat at /home/ec2-user/.julia/packages/LLVM/7Q46C/src/execution.jl:39\n\u001b[0m  convert(::Type{T}, \u001b[91m::LLVM.ConstantFP\u001b[39m) where T<:AbstractFloat at /home/ec2-user/.julia/packages/LLVM/7Q46C/src/core/value/constant.jl:98\n\u001b[0m  ...",
      "",
      "Stacktrace:",
      " [1] setindex!(A::Vector{Float64}, x::Vector{Float64}, i1::Int64)",
      "   @ Base ./array.jl:839",
      " [2] int2(x::Vector{Float64}, f::Vector{Float64})",
      "   @ Main ./In[24]:2",
      " [3] generic_integrand!(ndim::Int32, x_::Ptr{Float64}, ncomp::Int32, f_::Ptr{Float64}, func!::typeof(int2))",
      "   @ Cuba ~/.julia/packages/Cuba/KIQTB/src/Cuba.jl:92",
      " [4] dointegrate!",
      "   @ ~/.julia/packages/Cuba/KIQTB/src/suave.jl:43 [inlined]",
      " [5] dointegrate",
      "   @ ~/.julia/packages/Cuba/KIQTB/src/Cuba.jl:195 [inlined]",
      " [6] suave(integrand::typeof(int2), ndim::Int64, ncomp::Int64; nvec::Int64, rtol::Float64, atol::Float64, flags::Int64, seed::Int64, minevals::Int64, maxevals::Int64, nnew::Int64, nmin::Int64, flatness::Int64, statefile::String, spin::Ptr{Nothing}, reltol::Missing, abstol::Missing)",
      "   @ Cuba ~/.julia/packages/Cuba/KIQTB/src/suave.jl:104",
      " [7] top-level scope",
      "   @ show.jl:955",
      " [8] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [9] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1094"
     ]
    }
   ],
   "source": [
    "\n",
    "nvec=15000000\n",
    "maxevals=300000000\n",
    "nmin=2\n",
    "nnew=80000\n",
    "flatness=150\n",
    "\n",
    "@show result, err = suave(int, M, 1, atol=atol, rtol=rtol,maxevals=maxevals,nnew=nnew,nmin=nmin,flatness=flatness); \n",
    "@show result, err = suave(int2, M, 1, atol=atol, rtol=rtol,maxevals=maxevals,nnew=nnew,nmin=nmin,flatness=flatness); \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d4b8f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multithread\n",
    "function int_thread_col(x, f)\n",
    "    Threads.@threads for i in 1:size(x,2)\n",
    "      f[i] = pdf(Product(Beta.(1.0,2.0*ones(M))),@view(x[:,i]))\n",
    "    end\n",
    "end\n",
    "\n",
    "# multithread and loop to create product distribution\n",
    "function int_thread_el(x,f)\n",
    "   f[1,:] .= 1.0\n",
    "   Threads.@threads for j in 1:size(x,2)\n",
    "       for i in 1:size(x, 1)\n",
    "           f[1, j] *= pdf(Beta(1.0,2.0),@view(x[i,j]))\n",
    "       end\n",
    "   end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d02e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monte carlo suave\n",
    "\n",
    "nvec=15000000\n",
    "maxevals=300000000\n",
    "nmin=2\n",
    "nnew=80000\n",
    "flatness=150\n",
    "\n",
    "@show result, err = suave(int, M, 1, atol=atol, rtol=rtol,maxevals=maxevals,nnew=nnew,nmin=nmin,flatness=flatness); \n",
    "@show result, err = suave(int_thread_col, M, 1, atol=atol, rtol=rtol,maxevals=maxevals, nvec=nvec,nnew=nnew,nmin=nmin,flatness=flatness); \n",
    "@show result, err = suave(int_thread_el, M, 1, atol=atol, rtol=rtol,maxevals=maxevals, nvec=nvec,nnew=nnew,nmin=nmin,flatness=flatness);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f180fcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m Downloading\u001b[22m\u001b[39m artifact: CUDA110\n",
      "\u001b[32m\u001b[1m Downloading\u001b[22m\u001b[39m artifact: CUDNN_CUDA110\n",
      "\u001b[32m\u001b[1m Downloading\u001b[22m\u001b[39m artifact: CUTENSOR_CUDA110\n",
      "┌ Warning: Your Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "│ Some functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "└ @ CUDA /home/ec2-user/.julia/packages/CUDA/k52QH/src/state.jl:224\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5-element CuArray{Float64, 1}:\n",
       " 0.09862817519574185\n",
       " 0.37211988789676176\n",
       " 0.9038049126217622\n",
       " 0.2645842797714655\n",
       " 0.934906268138505"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# User Inputs\n",
    "M= 5 # number of independent uniform random variables\n",
    "atol=1e-10\n",
    "rtol=1e-10\n",
    "nvec=100000\n",
    "maxevals=10000\n",
    "\n",
    "# Initializing Matrices\n",
    "ones_mat = CuArray(ones(Float32, M))\n",
    "result = CUDA.ones(Float32, (nvec,1))\n",
    "x_cpu = rand(Float64, (M, nvec))\n",
    "x = CuArray(x_cpu)\n",
    "x_1d_cpu = rand(Float64, M)\n",
    "x_1d_gpu = CuArray(x_1d_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23eb4795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuArray{Float32, 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@btime suave($(int), $M, 1, atol=$atol, rtol=$rtol,maxevals=$maxevals, nnew=$nnew,nmin=$nmin,flatness=$flatness) # fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0b2ed96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuArray{Float64, 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typeof( CuArray(x_cpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d99f72ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m     Testing\u001b[22m\u001b[39m CUDA\n",
      "\u001b[32m\u001b[1m      Status\u001b[22m\u001b[39m `/tmp/jl_tUYAd6/Project.toml`\n",
      " \u001b[90m [79e6a3ab] \u001b[39m\u001b[37mAdapt v3.3.0\u001b[39m\n",
      " \u001b[90m [ab4f0b2a] \u001b[39m\u001b[37mBFloat16s v0.1.0\u001b[39m\n",
      " \u001b[90m [052768ef] \u001b[39m\u001b[37mCUDA v3.1.0\u001b[39m\n",
      " \u001b[90m [864edb3b] \u001b[39m\u001b[37mDataStructures v0.18.9\u001b[39m\n",
      " \u001b[90m [7a1cc6ca] \u001b[39m\u001b[37mFFTW v1.4.1\u001b[39m\n",
      " \u001b[90m [1a297f60] \u001b[39m\u001b[37mFillArrays v0.11.7\u001b[39m\n",
      " \u001b[90m [0c68f7d7] \u001b[39m\u001b[37mGPUArrays v6.2.2\u001b[39m\n",
      " \u001b[90m [a98d9a8b] \u001b[39m\u001b[37mInterpolations v0.13.2\u001b[39m\n",
      " \u001b[90m [872c559c] \u001b[39m\u001b[37mNNlib v0.7.19\u001b[39m\n",
      " \u001b[90m [ade2ca70] \u001b[39m\u001b[37mDates `@stdlib/Dates`\u001b[39m\n",
      " \u001b[90m [8ba89e20] \u001b[39m\u001b[37mDistributed `@stdlib/Distributed`\u001b[39m\n",
      " \u001b[90m [37e2e46d] \u001b[39m\u001b[37mLinearAlgebra `@stdlib/LinearAlgebra`\u001b[39m\n",
      " \u001b[90m [de0858da] \u001b[39m\u001b[37mPrintf `@stdlib/Printf`\u001b[39m\n",
      " \u001b[90m [3fa0cd96] \u001b[39m\u001b[37mREPL `@stdlib/REPL`\u001b[39m\n",
      " \u001b[90m [9a3f8284] \u001b[39m\u001b[37mRandom `@stdlib/Random`\u001b[39m\n",
      " \u001b[90m [2f01184e] \u001b[39m\u001b[37mSparseArrays `@stdlib/SparseArrays`\u001b[39m\n",
      " \u001b[90m [10745b16] \u001b[39m\u001b[37mStatistics `@stdlib/Statistics`\u001b[39m\n",
      " \u001b[90m [8dfed614] \u001b[39m\u001b[37mTest `@stdlib/Test`\u001b[39m\n",
      "\u001b[32m\u001b[1m      Status\u001b[22m\u001b[39m `/tmp/jl_tUYAd6/Manifest.toml`\n",
      " \u001b[90m [621f4979] \u001b[39m\u001b[37mAbstractFFTs v1.0.1\u001b[39m\n",
      " \u001b[90m [79e6a3ab] \u001b[39m\u001b[37mAdapt v3.3.0\u001b[39m\n",
      " \u001b[90m [13072b0f] \u001b[39m\u001b[37mAxisAlgorithms v1.0.0\u001b[39m\n",
      " \u001b[90m [ab4f0b2a] \u001b[39m\u001b[37mBFloat16s v0.1.0\u001b[39m\n",
      " \u001b[90m [fa961155] \u001b[39m\u001b[37mCEnum v0.4.1\u001b[39m\n",
      " \u001b[90m [052768ef] \u001b[39m\u001b[37mCUDA v3.1.0\u001b[39m\n",
      " \u001b[90m [d360d2e6] \u001b[39m\u001b[37mChainRulesCore v0.9.40\u001b[39m\n",
      " \u001b[90m [34da2185] \u001b[39m\u001b[37mCompat v3.27.0\u001b[39m\n",
      " \u001b[90m [864edb3b] \u001b[39m\u001b[37mDataStructures v0.18.9\u001b[39m\n",
      " \u001b[90m [e2ba6199] \u001b[39m\u001b[37mExprTools v0.1.3\u001b[39m\n",
      " \u001b[90m [7a1cc6ca] \u001b[39m\u001b[37mFFTW v1.4.1\u001b[39m\n",
      " \u001b[90m [1a297f60] \u001b[39m\u001b[37mFillArrays v0.11.7\u001b[39m\n",
      " \u001b[90m [0c68f7d7] \u001b[39m\u001b[37mGPUArrays v6.2.2\u001b[39m\n",
      " \u001b[90m [61eb1bfa] \u001b[39m\u001b[37mGPUCompiler v0.11.4\u001b[39m\n",
      " \u001b[90m [a98d9a8b] \u001b[39m\u001b[37mInterpolations v0.13.2\u001b[39m\n",
      " \u001b[90m [692b3bcd] \u001b[39m\u001b[37mJLLWrappers v1.3.0\u001b[39m\n",
      " \u001b[90m [929cbde3] \u001b[39m\u001b[37mLLVM v3.6.0\u001b[39m\n",
      " \u001b[90m [1914dd2f] \u001b[39m\u001b[37mMacroTools v0.5.6\u001b[39m\n",
      " \u001b[90m [c03570c3] \u001b[39m\u001b[37mMemoize v0.4.4\u001b[39m\n",
      " \u001b[90m [872c559c] \u001b[39m\u001b[37mNNlib v0.7.19\u001b[39m\n",
      " \u001b[90m [6fe1bfb0] \u001b[39m\u001b[37mOffsetArrays v1.8.0\u001b[39m\n",
      " \u001b[90m [bac558e1] \u001b[39m\u001b[37mOrderedCollections v1.4.0\u001b[39m\n",
      " \u001b[90m [21216c6a] \u001b[39m\u001b[37mPreferences v1.2.1\u001b[39m\n",
      " \u001b[90m [e6cf234a] \u001b[39m\u001b[37mRandomNumbers v1.4.0\u001b[39m\n",
      " \u001b[90m [c84ed2f1] \u001b[39m\u001b[37mRatios v0.4.0\u001b[39m\n",
      " \u001b[90m [189a3867] \u001b[39m\u001b[37mReexport v1.0.0\u001b[39m\n",
      " \u001b[90m [ae029012] \u001b[39m\u001b[37mRequires v1.1.3\u001b[39m\n",
      " \u001b[90m [6c6a2e73] \u001b[39m\u001b[37mScratch v1.0.3\u001b[39m\n",
      " \u001b[90m [276daf66] \u001b[39m\u001b[37mSpecialFunctions v1.3.0\u001b[39m\n",
      " \u001b[90m [90137ffa] \u001b[39m\u001b[37mStaticArrays v1.2.0\u001b[39m\n",
      " \u001b[90m [a759f4b9] \u001b[39m\u001b[37mTimerOutputs v0.5.8\u001b[39m\n",
      " \u001b[90m [efce3f68] \u001b[39m\u001b[37mWoodburyMatrices v0.5.3\u001b[39m\n",
      " \u001b[90m [f5851436] \u001b[39m\u001b[37mFFTW_jll v3.3.9+7\u001b[39m\n",
      " \u001b[90m [1d5cc7b8] \u001b[39m\u001b[37mIntelOpenMP_jll v2018.0.3+2\u001b[39m\n",
      " \u001b[90m [856f044c] \u001b[39m\u001b[37mMKL_jll v2021.1.1+1\u001b[39m\n",
      " \u001b[90m [efe28fd5] \u001b[39m\u001b[37mOpenSpecFun_jll v0.5.4+0\u001b[39m\n",
      " \u001b[90m [0dad84c5] \u001b[39m\u001b[37mArgTools `@stdlib/ArgTools`\u001b[39m\n",
      " \u001b[90m [56f22d72] \u001b[39m\u001b[37mArtifacts `@stdlib/Artifacts`\u001b[39m\n",
      " \u001b[90m [2a0f44e3] \u001b[39m\u001b[37mBase64 `@stdlib/Base64`\u001b[39m\n",
      " \u001b[90m [ade2ca70] \u001b[39m\u001b[37mDates `@stdlib/Dates`\u001b[39m\n",
      " \u001b[90m [8bb1440f] \u001b[39m\u001b[37mDelimitedFiles `@stdlib/DelimitedFiles`\u001b[39m\n",
      " \u001b[90m [8ba89e20] \u001b[39m\u001b[37mDistributed `@stdlib/Distributed`\u001b[39m\n",
      " \u001b[90m [f43a241f] \u001b[39m\u001b[37mDownloads `@stdlib/Downloads`\u001b[39m\n",
      " \u001b[90m [b77e0a4c] \u001b[39m\u001b[37mInteractiveUtils `@stdlib/InteractiveUtils`\u001b[39m\n",
      " \u001b[90m [4af54fe1] \u001b[39m\u001b[37mLazyArtifacts `@stdlib/LazyArtifacts`\u001b[39m\n",
      " \u001b[90m [b27032c2] \u001b[39m\u001b[37mLibCURL `@stdlib/LibCURL`\u001b[39m\n",
      " \u001b[90m [76f85450] \u001b[39m\u001b[37mLibGit2 `@stdlib/LibGit2`\u001b[39m\n",
      " \u001b[90m [8f399da3] \u001b[39m\u001b[37mLibdl `@stdlib/Libdl`\u001b[39m\n",
      " \u001b[90m [37e2e46d] \u001b[39m\u001b[37mLinearAlgebra `@stdlib/LinearAlgebra`\u001b[39m\n",
      " \u001b[90m [56ddb016] \u001b[39m\u001b[37mLogging `@stdlib/Logging`\u001b[39m\n",
      " \u001b[90m [d6f4376e] \u001b[39m\u001b[37mMarkdown `@stdlib/Markdown`\u001b[39m\n",
      " \u001b[90m [a63ad114] \u001b[39m\u001b[37mMmap `@stdlib/Mmap`\u001b[39m\n",
      " \u001b[90m [ca575930] \u001b[39m\u001b[37mNetworkOptions `@stdlib/NetworkOptions`\u001b[39m\n",
      " \u001b[90m [44cfe95a] \u001b[39m\u001b[37mPkg `@stdlib/Pkg`\u001b[39m\n",
      " \u001b[90m [de0858da] \u001b[39m\u001b[37mPrintf `@stdlib/Printf`\u001b[39m\n",
      " \u001b[90m [3fa0cd96] \u001b[39m\u001b[37mREPL `@stdlib/REPL`\u001b[39m\n",
      " \u001b[90m [9a3f8284] \u001b[39m\u001b[37mRandom `@stdlib/Random`\u001b[39m\n",
      " \u001b[90m [ea8e919c] \u001b[39m\u001b[37mSHA `@stdlib/SHA`\u001b[39m\n",
      " \u001b[90m [9e88b42a] \u001b[39m\u001b[37mSerialization `@stdlib/Serialization`\u001b[39m\n",
      " \u001b[90m [1a1011a3] \u001b[39m\u001b[37mSharedArrays `@stdlib/SharedArrays`\u001b[39m\n",
      " \u001b[90m [6462fe0b] \u001b[39m\u001b[37mSockets `@stdlib/Sockets`\u001b[39m\n",
      " \u001b[90m [2f01184e] \u001b[39m\u001b[37mSparseArrays `@stdlib/SparseArrays`\u001b[39m\n",
      " \u001b[90m [10745b16] \u001b[39m\u001b[37mStatistics `@stdlib/Statistics`\u001b[39m\n",
      " \u001b[90m [fa267f1f] \u001b[39m\u001b[37mTOML `@stdlib/TOML`\u001b[39m\n",
      " \u001b[90m [a4e569a6] \u001b[39m\u001b[37mTar `@stdlib/Tar`\u001b[39m\n",
      " \u001b[90m [8dfed614] \u001b[39m\u001b[37mTest `@stdlib/Test`\u001b[39m\n",
      " \u001b[90m [cf7118a7] \u001b[39m\u001b[37mUUIDs `@stdlib/UUIDs`\u001b[39m\n",
      " \u001b[90m [4ec0a83e] \u001b[39m\u001b[37mUnicode `@stdlib/Unicode`\u001b[39m\n",
      " \u001b[90m [e66e0078] \u001b[39m\u001b[37mCompilerSupportLibraries_jll `@stdlib/CompilerSupportLibraries_jll`\u001b[39m\n",
      " \u001b[90m [deac9b47] \u001b[39m\u001b[37mLibCURL_jll `@stdlib/LibCURL_jll`\u001b[39m\n",
      " \u001b[90m [29816b5a] \u001b[39m\u001b[37mLibSSH2_jll `@stdlib/LibSSH2_jll`\u001b[39m\n",
      " \u001b[90m [c8ffd9c3] \u001b[39m\u001b[37mMbedTLS_jll `@stdlib/MbedTLS_jll`\u001b[39m\n",
      " \u001b[90m [14a3606d] \u001b[39m\u001b[37mMozillaCACerts_jll `@stdlib/MozillaCACerts_jll`\u001b[39m\n",
      " \u001b[90m [83775a58] \u001b[39m\u001b[37mZlib_jll `@stdlib/Zlib_jll`\u001b[39m\n",
      " \u001b[90m [8e850ede] \u001b[39m\u001b[37mnghttp2_jll `@stdlib/nghttp2_jll`\u001b[39m\n",
      " \u001b[90m [3f19e933] \u001b[39m\u001b[37mp7zip_jll `@stdlib/p7zip_jll`\u001b[39m\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mRatios\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mWoodburyMatrices\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mOffsetArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mAxisAlgorithms\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mIntelOpenMP_jll\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mFFTW_jll\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMKL_jll\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mNNlib\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStaticArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mFFTW\n",
      "\u001b[32m  ✓ \u001b[39mInterpolations\n",
      "11 dependencies successfully precompiled in 12 seconds (29 already precompiled)\n",
      "\u001b[32m\u001b[1m     Testing\u001b[22m\u001b[39m Running tests...\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSystem information:\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mCUDA toolkit 11.0.3, artifact installation\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mCUDA driver 11.0.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mNVIDIA driver 450.119.3\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mLibraries: \n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- CUBLAS: 11.2.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- CURAND: 10.2.1\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- CUFFT: 10.2.1\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- CUSOLVER: 10.6.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- CUSPARSE: 11.1.1\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- CUPTI: 13.0.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- NVML: 11.0.0+450.119.3\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- CUDNN: 8.10.0 (for CUDA 11.2.0)\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- CUTENSOR: 1.2.2 (for CUDA 11.1.0)\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39mToolchain:\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- Julia: 1.6.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- LLVM: 11.0.1\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- PTX ISA support: 3.2, 4.0, 4.1, 4.2, 4.3, 5.0, 6.0, 6.1, 6.3, 6.4, 6.5, 7.0\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m- Device support: sm_35, sm_37, sm_50, sm_52, sm_53, sm_60, sm_61, sm_62, sm_70, sm_72, sm_75, sm_80\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m\n",
      "\u001b[36m\u001b[1m│ \u001b[22m\u001b[39m1 device:\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39m  0: Tesla K80 (sm_37, 11.005 GiB / 11.173 GiB available)\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mTesting using 1 device(s): 1. Tesla K80 (UUID 9328f6ae-24f2-c28c-4860-2965d0547036)\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSkipping the following tests: cutensor/base, cutensor/contractions, cutensor/elementwise_binary, cutensor/elementwise_trinary, cutensor/permutations, cutensor/reductions, device/random, device/wmma\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m                                          | \u001b[37m         | ---------------- GPU ---------------- | ---------------- CPU ---------------- |\u001b[39m\n",
      "\u001b[37mTest\u001b[39m\u001b[37m                             (Worker) | \u001b[39m\u001b[37mTime (s) | GC (s) | GC % | Alloc (MB) | RSS (MB) | GC (s) | GC % | Alloc (MB) | RSS (MB) |\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mapiutils\u001b[39m\u001b[37m                              (3) | \u001b[39m\u001b[37m    2.31 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   56.25 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      5.53 | \u001b[39m\u001b[37m  838.73 |\u001b[39m\n",
      "\u001b[37mnvtx\u001b[39m\u001b[37m                                 (16) | \u001b[39m\u001b[37m    3.08 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   56.25 | \u001b[39m\u001b[37m  0.09 | \u001b[39m\u001b[37m 2.9 | \u001b[39m\u001b[37m     31.42 | \u001b[39m\u001b[37m  838.73 |\u001b[39m\n",
      "\u001b[37mnvml\u001b[39m\u001b[37m                                 (15) | \u001b[39m\u001b[37m    6.13 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   56.25 | \u001b[39m\u001b[37m  0.11 | \u001b[39m\u001b[37m 1.8 | \u001b[39m\u001b[37m     50.11 | \u001b[39m\u001b[37m  838.73 |\u001b[39m\n",
      "\u001b[37mcurand\u001b[39m\u001b[37m                                (9) | \u001b[39m\u001b[37m    5.89 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   60.25 | \u001b[39m\u001b[37m  0.18 | \u001b[39m\u001b[37m 3.1 | \u001b[39m\u001b[37m     48.56 | \u001b[39m\u001b[37m  838.73 |\u001b[39m\n",
      "\u001b[37mpointer\u001b[39m\u001b[37m                              (17) | \u001b[39m\u001b[37m   13.52 | \u001b[39m\u001b[37m  6.10 | \u001b[39m\u001b[37m45.1 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   57.25 | \u001b[39m\u001b[37m  0.27 | \u001b[39m\u001b[37m 2.0 | \u001b[39m\u001b[37m    136.23 | \u001b[39m\u001b[37m  838.73 |\u001b[39m\n",
      "\u001b[37minitialization\u001b[39m\u001b[37m                        (2) | \u001b[39m\u001b[37m   53.66 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   56.25 | \u001b[39m\u001b[37m  0.89 | \u001b[39m\u001b[37m 1.7 | \u001b[39m\u001b[37m    429.70 | \u001b[39m\u001b[37m  838.73 |\u001b[39m\n",
      "      From worker 11:\t\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "      From worker 11:\t\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "      From worker 11:\t\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[37miterator\u001b[39m\u001b[37m                             (14) | \u001b[39m\u001b[37m   46.43 | \u001b[39m\u001b[37m  5.51 | \u001b[39m\u001b[37m11.9 | \u001b[39m\u001b[37m      1.16 | \u001b[39m\u001b[37m   58.25 | \u001b[39m\u001b[37m  1.35 | \u001b[39m\u001b[37m 2.9 | \u001b[39m\u001b[37m    626.65 | \u001b[39m\u001b[37m  838.73 |\u001b[39m\n",
      "\u001b[37mutils\u001b[39m\u001b[37m                                (14) | \u001b[39m\u001b[37m   15.53 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m  102.25 | \u001b[39m\u001b[37m  0.36 | \u001b[39m\u001b[37m 2.3 | \u001b[39m\u001b[37m    164.49 | \u001b[39m\u001b[37m  838.73 |\u001b[39m\n",
      "\u001b[37mcudadrv/context\u001b[39m\u001b[37m                      (14) | \u001b[39m\u001b[37m    2.30 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   56.25 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      8.63 | \u001b[39m\u001b[37m  862.75 |\u001b[39m\n",
      "\u001b[37mpool\u001b[39m\u001b[37m                                  (3) | \u001b[39m\u001b[37m   60.93 | \u001b[39m\u001b[37m  0.21 | \u001b[39m\u001b[37m 0.3 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   56.25 | \u001b[39m\u001b[37m  3.83 | \u001b[39m\u001b[37m 6.3 | \u001b[39m\u001b[37m    947.06 | \u001b[39m\u001b[37m  838.73 |\u001b[39m\n",
      "\u001b[37mcudadrv/devices\u001b[39m\u001b[37m                      (14) | \u001b[39m\u001b[37m    3.49 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   56.25 | \u001b[39m\u001b[37m  0.08 | \u001b[39m\u001b[37m 2.3 | \u001b[39m\u001b[37m     38.60 | \u001b[39m\u001b[37m  862.75 |\u001b[39m\n",
      "\u001b[37mcudadrv/errors\u001b[39m\u001b[37m                        (3) | \u001b[39m\u001b[37m    1.31 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   56.25 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m     10.76 | \u001b[39m\u001b[37m  838.73 |\u001b[39m\n",
      "\u001b[37mcudadrv/events\u001b[39m\u001b[37m                       (14) | \u001b[39m\u001b[37m    0.82 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   56.25 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      4.58 | \u001b[39m\u001b[37m  862.75 |\u001b[39m\n",
      "\u001b[37mcudadrv/execution\u001b[39m\u001b[37m                     (3) | \u001b[39m\u001b[37m   28.47 | \u001b[39m\u001b[37m  1.51 | \u001b[39m\u001b[37m 5.3 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   57.25 | \u001b[39m\u001b[37m  0.76 | \u001b[39m\u001b[37m 2.7 | \u001b[39m\u001b[37m    384.47 | \u001b[39m\u001b[37m  838.73 |\u001b[39m\n",
      "\u001b[37mcudadrv/module\u001b[39m\u001b[37m                        (3) | \u001b[39m\u001b[37m    8.11 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   56.25 | \u001b[39m\u001b[37m  0.16 | \u001b[39m\u001b[37m 1.9 | \u001b[39m\u001b[37m     67.93 | \u001b[39m\u001b[37m  838.73 |\u001b[39m\n",
      "\u001b[37mcudadrv/occupancy\u001b[39m\u001b[37m                     (3) | \u001b[39m\u001b[37m    1.39 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   56.25 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      8.69 | \u001b[39m\u001b[37m  838.73 |\u001b[39m\n",
      "\u001b[31mcudadrv/pool\u001b[39m\u001b[31m                          (3) |         failed at 2021-05-15T01:37:41.036\u001b[39m\n",
      "      From worker 11:\t\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "      From worker 11:\t\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "      From worker 11:\t\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[37mthreading\u001b[39m\u001b[37m                             (2) | \u001b[39m\u001b[37m  173.31 | \u001b[39m\u001b[37m  0.22 | \u001b[39m\u001b[37m 0.1 | \u001b[39m\u001b[37m     10.94 | \u001b[39m\u001b[37m  110.75 | \u001b[39m\u001b[37m  5.51 | \u001b[39m\u001b[37m 3.2 | \u001b[39m\u001b[37m   1762.61 | \u001b[39m\u001b[37m  901.81 |\u001b[39m\n",
      "\u001b[37mcudadrv/memory\u001b[39m\u001b[37m                       (14) | \u001b[39m\u001b[37m  142.34 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.02 | \u001b[39m\u001b[37m   61.75 | \u001b[39m\u001b[37m  3.78 | \u001b[39m\u001b[37m 2.7 | \u001b[39m\u001b[37m   1260.19 | \u001b[39m\u001b[37m  862.75 |\u001b[39m\n",
      "\u001b[37mcudadrv/profile\u001b[39m\u001b[37m                       (2) | \u001b[39m\u001b[37m    2.72 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   56.25 | \u001b[39m\u001b[37m  0.07 | \u001b[39m\u001b[37m 2.6 | \u001b[39m\u001b[37m     51.77 | \u001b[39m\u001b[37m  901.81 |\u001b[39m\n",
      "\u001b[37mcudadrv/stream\u001b[39m\u001b[37m                       (14) | \u001b[39m\u001b[37m    0.92 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   56.25 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      6.11 | \u001b[39m\u001b[37m  862.75 |\u001b[39m\n",
      "\u001b[37mcudadrv/version\u001b[39m\u001b[37m                       (2) | \u001b[39m\u001b[37m    0.21 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   56.25 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.08 | \u001b[39m\u001b[37m  901.81 |\u001b[39m\n",
      "\u001b[37mcudnn/convolution\u001b[39m\u001b[37m                     (2) | \u001b[39m\u001b[37m    0.57 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   56.25 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      3.34 | \u001b[39m\u001b[37m  901.81 |\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mCUDA.CUDNN.cudnnDropoutSeed[] >= 0: dropout operations will be deterministic but 40x more expensive\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA.CUDNN ~/.julia/packages/CUDA/k52QH/lib/cudnn/dropout.jl:40\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mcudnn/activation\u001b[39m\u001b[37m                     (14) | \u001b[39m\u001b[37m   20.18 | \u001b[39m\u001b[37m  0.03 | \u001b[39m\u001b[37m 0.1 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m  109.25 | \u001b[39m\u001b[37m  0.71 | \u001b[39m\u001b[37m 3.5 | \u001b[39m\u001b[37m    248.54 | \u001b[39m\u001b[37m  956.41 |\u001b[39m\n",
      "\u001b[37mcodegen\u001b[39m\u001b[37m                               (6) | \u001b[39m\u001b[37m  254.56 | \u001b[39m\u001b[37m  5.74 | \u001b[39m\u001b[37m 2.3 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   97.12 | \u001b[39m\u001b[37m  6.05 | \u001b[39m\u001b[37m 2.4 | \u001b[39m\u001b[37m   2547.68 | \u001b[39m\u001b[37m  838.73 |\u001b[39m\n",
      "\u001b[37mcudnn/dropout\u001b[39m\u001b[37m                         (2) | \u001b[39m\u001b[37m   47.67 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.63 | \u001b[39m\u001b[37m  115.75 | \u001b[39m\u001b[37m  1.30 | \u001b[39m\u001b[37m 2.7 | \u001b[39m\u001b[37m    516.37 | \u001b[39m\u001b[37m  969.43 |\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mrandom\u001b[39m\u001b[37m                               (16) | \u001b[39m\u001b[37m  277.55 | \u001b[39m\u001b[37m  5.32 | \u001b[39m\u001b[37m 1.9 | \u001b[39m\u001b[37m      0.02 | \u001b[39m\u001b[37m   63.75 | \u001b[39m\u001b[37m  7.39 | \u001b[39m\u001b[37m 2.7 | \u001b[39m\u001b[37m   2892.02 | \u001b[39m\u001b[37m  838.73 |\u001b[39m\n",
      "\u001b[37mcudnn/optensor\u001b[39m\u001b[37m                        (2) | \u001b[39m\u001b[37m   18.73 | \u001b[39m\u001b[37m  0.05 | \u001b[39m\u001b[37m 0.3 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m  109.25 | \u001b[39m\u001b[37m  0.72 | \u001b[39m\u001b[37m 3.8 | \u001b[39m\u001b[37m    241.95 | \u001b[39m\u001b[37m  978.89 |\u001b[39m\n",
      "\u001b[37mcudnn/reduce\u001b[39m\u001b[37m                          (2) | \u001b[39m\u001b[37m   28.25 | \u001b[39m\u001b[37m  0.02 | \u001b[39m\u001b[37m 0.1 | \u001b[39m\u001b[37m      0.02 | \u001b[39m\u001b[37m  109.25 | \u001b[39m\u001b[37m  1.12 | \u001b[39m\u001b[37m 4.0 | \u001b[39m\u001b[37m    371.64 | \u001b[39m\u001b[37m  985.86 |\u001b[39m\n",
      "\u001b[37mcudnn/inplace\u001b[39m\u001b[37m                        (18) | \u001b[39m\u001b[37m   36.06 | \u001b[39m\u001b[37m  5.44 | \u001b[39m\u001b[37m15.1 | \u001b[39m\u001b[37m      0.01 | \u001b[39m\u001b[37m  111.25 | \u001b[39m\u001b[37m  0.92 | \u001b[39m\u001b[37m 2.5 | \u001b[39m\u001b[37m    392.57 | \u001b[39m\u001b[37m  871.72 |\u001b[39m\n",
      "\u001b[37mcudnn/softmax\u001b[39m\u001b[37m                        (18) | \u001b[39m\u001b[37m   22.38 | \u001b[39m\u001b[37m  0.01 | \u001b[39m\u001b[37m 0.1 | \u001b[39m\u001b[37m      0.01 | \u001b[39m\u001b[37m  109.25 | \u001b[39m\u001b[37m  0.89 | \u001b[39m\u001b[37m 4.0 | \u001b[39m\u001b[37m    268.38 | \u001b[39m\u001b[37m  871.72 |\u001b[39m\n",
      "\u001b[37mcudnn/pooling\u001b[39m\u001b[37m                        (16) | \u001b[39m\u001b[37m   67.83 | \u001b[39m\u001b[37m  0.01 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.06 | \u001b[39m\u001b[37m  109.25 | \u001b[39m\u001b[37m  2.39 | \u001b[39m\u001b[37m 3.5 | \u001b[39m\u001b[37m    898.57 | \u001b[39m\u001b[37m  931.63 |\u001b[39m\n",
      "\u001b[37mcudnn/tensor\u001b[39m\u001b[37m                         (18) | \u001b[39m\u001b[37m    8.89 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   60.25 | \u001b[39m\u001b[37m  1.08 | \u001b[39m\u001b[37m12.2 | \u001b[39m\u001b[37m     92.67 | \u001b[39m\u001b[37m  871.72 |\u001b[39m\n",
      "      From worker 11:\t\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "      From worker 11:\t\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "      From worker 11:\t\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[37mcudnn/multiheadattn\u001b[39m\u001b[37m                  (14) | \u001b[39m\u001b[37m  155.29 | \u001b[39m\u001b[37m  0.01 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.14 | \u001b[39m\u001b[37m  161.75 | \u001b[39m\u001b[37m  5.47 | \u001b[39m\u001b[37m 3.5 | \u001b[39m\u001b[37m   1996.75 | \u001b[39m\u001b[37m 1173.73 |\u001b[39m\n",
      "      From worker 13:\tWARNING: Method definition #1719#kernel(Any) in module Main at /home/ec2-user/.julia/packages/CUDA/k52QH/test/execution.jl:316 overwritten at /home/ec2-user/.julia/packages/CUDA/k52QH/test/execution.jl:324.\n",
      "\u001b[31mcudnn/rnn\u001b[39m\u001b[31m                             (2) |         failed at 2021-05-15T01:42:34.551\u001b[39m\n",
      "\u001b[37mcudnn/normalization\u001b[39m\u001b[37m                   (6) | \u001b[39m\u001b[37m  219.59 | \u001b[39m\u001b[37m  0.02 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.11 | \u001b[39m\u001b[37m  173.75 | \u001b[39m\u001b[37m  6.87 | \u001b[39m\u001b[37m 3.1 | \u001b[39m\u001b[37m   2263.65 | \u001b[39m\u001b[37m 1352.54 |\u001b[39m\n",
      "\u001b[37mbroadcast\u001b[39m\u001b[37m                             (5) | \u001b[39m\u001b[37m  490.93 | \u001b[39m\u001b[37m  6.29 | \u001b[39m\u001b[37m 1.3 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   61.75 | \u001b[39m\u001b[37m 12.52 | \u001b[39m\u001b[37m 2.6 | \u001b[39m\u001b[37m   4829.63 | \u001b[39m\u001b[37m  849.91 |\u001b[39m\n",
      "\u001b[37mcufft\u001b[39m\u001b[37m                                 (8) | \u001b[39m\u001b[37m  540.96 | \u001b[39m\u001b[37m  5.67 | \u001b[39m\u001b[37m 1.0 | \u001b[39m\u001b[37m    110.15 | \u001b[39m\u001b[37m  154.75 | \u001b[39m\u001b[37m 11.93 | \u001b[39m\u001b[37m 2.2 | \u001b[39m\u001b[37m   4825.51 | \u001b[39m\u001b[37m 1048.38 |\u001b[39m\n",
      "\u001b[37mdevice/array\u001b[39m\u001b[37m                          (5) | \u001b[39m\u001b[37m   53.15 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   61.75 | \u001b[39m\u001b[37m  1.49 | \u001b[39m\u001b[37m 2.8 | \u001b[39m\u001b[37m    616.80 | \u001b[39m\u001b[37m  852.09 |\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mgpuarrays/math\u001b[39m\u001b[37m                        (5) | \u001b[39m\u001b[37m   53.04 | \u001b[39m\u001b[37m  0.02 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   61.75 | \u001b[39m\u001b[37m  1.78 | \u001b[39m\u001b[37m 3.4 | \u001b[39m\u001b[37m    695.13 | \u001b[39m\u001b[37m  870.58 |\u001b[39m\n",
      "\u001b[37mcusolver/sparse\u001b[39m\u001b[37m                      (14) | \u001b[39m\u001b[37m  215.37 | \u001b[39m\u001b[37m  0.01 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.19 | \u001b[39m\u001b[37m  202.75 | \u001b[39m\u001b[37m  6.25 | \u001b[39m\u001b[37m 2.9 | \u001b[39m\u001b[37m   2323.41 | \u001b[39m\u001b[37m 1390.13 |\u001b[39m\n",
      "      From worker 11:\t\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "      From worker 11:\t\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "      From worker 11:\t\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[37mgpuarrays/input output\u001b[39m\u001b[37m               (14) | \u001b[39m\u001b[37m   29.56 | \u001b[39m\u001b[37m  0.02 | \u001b[39m\u001b[37m 0.1 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   57.25 | \u001b[39m\u001b[37m  2.45 | \u001b[39m\u001b[37m 8.3 | \u001b[39m\u001b[37m    249.41 | \u001b[39m\u001b[37m 1390.13 |\u001b[39m\n",
      "\u001b[37mcusolver/multigpu\u001b[39m\u001b[37m                    (18) | \u001b[39m\u001b[37m  329.27 | \u001b[39m\u001b[37m  0.22 | \u001b[39m\u001b[37m 0.1 | \u001b[39m\u001b[37m    431.84 | \u001b[39m\u001b[37m 1356.75 | \u001b[39m\u001b[37m  9.82 | \u001b[39m\u001b[37m 3.0 | \u001b[39m\u001b[37m   3569.91 | \u001b[39m\u001b[37m 1068.85 |\u001b[39m\n",
      "\u001b[37mdevice/ldg\u001b[39m\u001b[37m                            (8) | \u001b[39m\u001b[37m  150.86 | \u001b[39m\u001b[37m  0.02 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   61.75 | \u001b[39m\u001b[37m  4.07 | \u001b[39m\u001b[37m 2.7 | \u001b[39m\u001b[37m   1874.04 | \u001b[39m\u001b[37m 1048.38 |\u001b[39m\n",
      "\u001b[37mstatistics\u001b[39m\u001b[37m                            (9) | \u001b[39m\u001b[37m  714.66 | \u001b[39m\u001b[37m  5.64 | \u001b[39m\u001b[37m 0.8 | \u001b[39m\u001b[37m      1.21 | \u001b[39m\u001b[37m  106.75 | \u001b[39m\u001b[37m 22.30 | \u001b[39m\u001b[37m 3.1 | \u001b[39m\u001b[37m   7300.31 | \u001b[39m\u001b[37m 1045.39 |\u001b[39m\n",
      "\u001b[37mcusparse\u001b[39m\u001b[37m                             (10) | \u001b[39m\u001b[37m  736.96 | \u001b[39m\u001b[37m  5.57 | \u001b[39m\u001b[37m 0.8 | \u001b[39m\u001b[37m      8.45 | \u001b[39m\u001b[37m  157.75 | \u001b[39m\u001b[37m 18.51 | \u001b[39m\u001b[37m 2.5 | \u001b[39m\u001b[37m   7059.70 | \u001b[39m\u001b[37m 1124.05 |\u001b[39m\n",
      "\u001b[37mgpuarrays/indexing scalar\u001b[39m\u001b[37m             (5) | \u001b[39m\u001b[37m  144.25 | \u001b[39m\u001b[37m  0.03 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   61.75 | \u001b[39m\u001b[37m  4.73 | \u001b[39m\u001b[37m 3.3 | \u001b[39m\u001b[37m   1661.09 | \u001b[39m\u001b[37m  870.58 |\u001b[39m\n",
      "\u001b[37mcusparse/interfaces\u001b[39m\u001b[37m                   (6) | \u001b[39m\u001b[37m  283.95 | \u001b[39m\u001b[37m  0.04 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.24 | \u001b[39m\u001b[37m  118.75 | \u001b[39m\u001b[37m  6.36 | \u001b[39m\u001b[37m 2.2 | \u001b[39m\u001b[37m   2764.64 | \u001b[39m\u001b[37m 1414.07 |\u001b[39m\n",
      "\u001b[37mgpuarrays/interface\u001b[39m\u001b[37m                   (8) | \u001b[39m\u001b[37m   77.41 | \u001b[39m\u001b[37m  0.03 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   61.75 | \u001b[39m\u001b[37m  2.31 | \u001b[39m\u001b[37m 3.0 | \u001b[39m\u001b[37m    804.62 | \u001b[39m\u001b[37m 1048.38 |\u001b[39m\n",
      "\u001b[37mgpuarrays/constructors\u001b[39m\u001b[37m                (8) | \u001b[39m\u001b[37m   18.66 | \u001b[39m\u001b[37m  0.04 | \u001b[39m\u001b[37m 0.2 | \u001b[39m\u001b[37m      0.03 | \u001b[39m\u001b[37m   57.25 | \u001b[39m\u001b[37m  0.25 | \u001b[39m\u001b[37m 1.4 | \u001b[39m\u001b[37m    116.78 | \u001b[39m\u001b[37m 1048.38 |\u001b[39m\n",
      "      From worker 11:\t\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "      From worker 11:\t\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "      From worker 11:\t\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[37mgpuarrays/conversions\u001b[39m\u001b[37m                 (6) | \u001b[39m\u001b[37m   53.35 | \u001b[39m\u001b[37m  0.03 | \u001b[39m\u001b[37m 0.1 | \u001b[39m\u001b[37m      0.01 | \u001b[39m\u001b[37m   65.25 | \u001b[39m\u001b[37m  1.43 | \u001b[39m\u001b[37m 2.7 | \u001b[39m\u001b[37m    603.46 | \u001b[39m\u001b[37m 1414.07 |\u001b[39m\n",
      "\u001b[37mtexture\u001b[39m\u001b[37m                              (17) | \u001b[39m\u001b[37m  799.90 | \u001b[39m\u001b[37m  0.04 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.09 | \u001b[39m\u001b[37m   67.75 | \u001b[39m\u001b[37m 23.61 | \u001b[39m\u001b[37m 3.0 | \u001b[39m\u001b[37m   9088.41 | \u001b[39m\u001b[37m  893.31 |\u001b[39m\n",
      "\u001b[37mgpuarrays/value constructors\u001b[39m\u001b[37m         (14) | \u001b[39m\u001b[37m  187.16 | \u001b[39m\u001b[37m  0.03 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   61.75 | \u001b[39m\u001b[37m  5.12 | \u001b[39m\u001b[37m 2.7 | \u001b[39m\u001b[37m   2015.91 | \u001b[39m\u001b[37m 1390.13 |\u001b[39m\n",
      "\u001b[37mgpuarrays/iterator constructors\u001b[39m\u001b[37m       (9) | \u001b[39m\u001b[37m  145.74 | \u001b[39m\u001b[37m  0.02 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.02 | \u001b[39m\u001b[37m   61.75 | \u001b[39m\u001b[37m  5.87 | \u001b[39m\u001b[37m 4.0 | \u001b[39m\u001b[37m   1560.80 | \u001b[39m\u001b[37m 1101.14 |\u001b[39m\n",
      "\u001b[37mexceptions\u001b[39m\u001b[37m                           (12) | \u001b[39m\u001b[37m  903.87 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   56.25 | \u001b[39m\u001b[37m  0.40 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m    175.34 | \u001b[39m\u001b[37m  838.73 |\u001b[39m\n",
      "\u001b[37mgpuarrays/uniformscaling\u001b[39m\u001b[37m             (10) | \u001b[39m\u001b[37m  204.39 | \u001b[39m\u001b[37m  0.02 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.01 | \u001b[39m\u001b[37m   61.75 | \u001b[39m\u001b[37m  4.29 | \u001b[39m\u001b[37m 2.1 | \u001b[39m\u001b[37m   1581.77 | \u001b[39m\u001b[37m 1124.38 |\u001b[39m\n",
      "      From worker 11:\t\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "      From worker 11:\t\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "      From worker 11:\t\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "      From worker 11:\t\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "      From worker 11:\t\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "      From worker 11:\t\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n",
      "\u001b[37mgpuarrays/indexing multidimensional\u001b[39m\u001b[37m  (18) | \u001b[39m\u001b[37m  377.81 | \u001b[39m\u001b[37m  0.03 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.69 | \u001b[39m\u001b[37m   61.75 | \u001b[39m\u001b[37m 12.24 | \u001b[39m\u001b[37m 3.2 | \u001b[39m\u001b[37m   5010.78 | \u001b[39m\u001b[37m 1117.20 |\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mYour Tesla K80 GPU does not meet the minimal required compute capability (3.7.0 < 5.0).\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mSome functionality might not work. For a fully-supported set-up, please use an older version of CUDA.jl\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ CUDA ~/.julia/packages/CUDA/k52QH/src/state.jl:224\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37mexamples\u001b[39m\u001b[37m                             (11) | \u001b[39m\u001b[37m 1088.26 | \u001b[39m\u001b[37m  0.00 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.00 | \u001b[39m\u001b[37m   56.25 | \u001b[39m\u001b[37m  0.09 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m     44.51 | \u001b[39m\u001b[37m  838.73 |\u001b[39m\n",
      "\u001b[37mgpuarrays/random\u001b[39m\u001b[37m                      (8) | \u001b[39m\u001b[37m  300.52 | \u001b[39m\u001b[37m  0.03 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      0.03 | \u001b[39m\u001b[37m   63.75 | \u001b[39m\u001b[37m  8.42 | \u001b[39m\u001b[37m 2.8 | \u001b[39m\u001b[37m   3382.40 | \u001b[39m\u001b[37m 1052.46 |\u001b[39m\n",
      "\u001b[37mexecution\u001b[39m\u001b[37m                            (13) | \u001b[39m\u001b[37m 1131.12 | \u001b[39m\u001b[37m  5.81 | \u001b[39m\u001b[37m 0.5 | \u001b[39m\u001b[37m      0.02 | \u001b[39m\u001b[37m  109.62 | \u001b[39m\u001b[37m 31.81 | \u001b[39m\u001b[37m 2.8 | \u001b[39m\u001b[37m  12604.10 | \u001b[39m\u001b[37m 1003.78 |\u001b[39m\n",
      "\u001b[37mgpuarrays/base\u001b[39m\u001b[37m                        (6) | \u001b[39m\u001b[37m  315.41 | \u001b[39m\u001b[37m  0.01 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m     17.44 | \u001b[39m\u001b[37m  101.75 | \u001b[39m\u001b[37m 13.27 | \u001b[39m\u001b[37m 4.2 | \u001b[39m\u001b[37m   4701.24 | \u001b[39m\u001b[37m 1510.96 |\u001b[39m\n",
      "\u001b[37mcublas\u001b[39m\u001b[37m                                (7) | \u001b[39m\u001b[37m 1197.80 | \u001b[39m\u001b[37m  5.79 | \u001b[39m\u001b[37m 0.5 | \u001b[39m\u001b[37m     14.49 | \u001b[39m\u001b[37m  129.75 | \u001b[39m\u001b[37m 33.81 | \u001b[39m\u001b[37m 2.8 | \u001b[39m\u001b[37m  14570.33 | \u001b[39m\u001b[37m 1152.14 |\u001b[39m\n",
      "\u001b[37marray\u001b[39m\u001b[37m                                 (4) | \u001b[39m\u001b[37m 1219.01 | \u001b[39m\u001b[37m  6.64 | \u001b[39m\u001b[37m 0.5 | \u001b[39m\u001b[37m      5.29 | \u001b[39m\u001b[37m   82.88 | \u001b[39m\u001b[37m 41.76 | \u001b[39m\u001b[37m 3.4 | \u001b[39m\u001b[37m  14831.12 | \u001b[39m\u001b[37m 1005.93 |\u001b[39m\n",
      "\u001b[37mcusolver/dense\u001b[39m\u001b[37m                       (16) | \u001b[39m\u001b[37m  886.27 | \u001b[39m\u001b[37m  0.67 | \u001b[39m\u001b[37m 0.1 | \u001b[39m\u001b[37m   1294.29 | \u001b[39m\u001b[37m  242.75 | \u001b[39m\u001b[37m 29.91 | \u001b[39m\u001b[37m 3.4 | \u001b[39m\u001b[37m  12983.83 | \u001b[39m\u001b[37m 1535.34 |\u001b[39m\n",
      "\u001b[37msorting\u001b[39m\u001b[37m                              (15) | \u001b[39m\u001b[37m 1390.19 | \u001b[39m\u001b[37m  5.76 | \u001b[39m\u001b[37m 0.4 | \u001b[39m\u001b[37m    401.81 | \u001b[39m\u001b[37m  784.62 | \u001b[39m\u001b[37m 38.44 | \u001b[39m\u001b[37m 2.8 | \u001b[39m\u001b[37m  21920.00 | \u001b[39m\u001b[37m 5254.61 |\u001b[39m\n",
      "\u001b[37mgpuarrays/broadcasting\u001b[39m\u001b[37m               (14) | \u001b[39m\u001b[37m  664.06 | \u001b[39m\u001b[37m  0.03 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      1.19 | \u001b[39m\u001b[37m   62.75 | \u001b[39m\u001b[37m 23.93 | \u001b[39m\u001b[37m 3.6 | \u001b[39m\u001b[37m  15651.85 | \u001b[39m\u001b[37m 1480.42 |\u001b[39m\n",
      "\u001b[37mgpuarrays/linear algebra\u001b[39m\u001b[37m              (5) | \u001b[39m\u001b[37m  820.95 | \u001b[39m\u001b[37m  0.02 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      1.24 | \u001b[39m\u001b[37m  108.75 | \u001b[39m\u001b[37m 23.19 | \u001b[39m\u001b[37m 2.8 | \u001b[39m\u001b[37m  20868.96 | \u001b[39m\u001b[37m 1329.57 |\u001b[39m\n",
      "\u001b[37mgpuarrays/mapreduce essentials\u001b[39m\u001b[37m       (17) | \u001b[39m\u001b[37m  796.05 | \u001b[39m\u001b[37m  0.02 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      3.19 | \u001b[39m\u001b[37m   62.75 | \u001b[39m\u001b[37m 25.99 | \u001b[39m\u001b[37m 3.3 | \u001b[39m\u001b[37m  20604.40 | \u001b[39m\u001b[37m 1250.74 |\u001b[39m\n",
      "\u001b[37mdevice/intrinsics\u001b[39m\u001b[37m                    (19) | \u001b[39m\u001b[37m 1151.28 | \u001b[39m\u001b[37m  6.34 | \u001b[39m\u001b[37m 0.6 | \u001b[39m\u001b[37m      0.01 | \u001b[39m\u001b[37m  287.25 | \u001b[39m\u001b[37m 30.28 | \u001b[39m\u001b[37m 2.6 | \u001b[39m\u001b[37m  29594.60 | \u001b[39m\u001b[37m 1520.47 |\u001b[39m\n",
      "\u001b[37mgpuarrays/mapreduce derivatives\u001b[39m\u001b[37m       (9) | \u001b[39m\u001b[37m  993.08 | \u001b[39m\u001b[37m  0.04 | \u001b[39m\u001b[37m 0.0 | \u001b[39m\u001b[37m      3.06 | \u001b[39m\u001b[37m   64.75 | \u001b[39m\u001b[37m 27.24 | \u001b[39m\u001b[37m 2.7 | \u001b[39m\u001b[37m  30563.26 | \u001b[39m\u001b[37m 2209.12 |\u001b[39m\n",
      "Worker 3 failed running test cudadrv/pool:\n",
      "\u001b[91mSome tests did not pass: 0 passed, 0 failed, 1 errored, 0 broken.\u001b[39m\n",
      "\u001b[37mcudadrv/pool: \u001b[39m\u001b[91m\u001b[1mError During Test\u001b[22m\u001b[39m at \u001b[39m\u001b[1m/home/ec2-user/.julia/packages/CUDA/k52QH/test/setup.jl:56\u001b[22m\n",
      "  Got exception outside of a @test\n",
      "  LoadError: CUDA error: invalid argument (code 1, ERROR_INVALID_VALUE)\n",
      "  Stacktrace:\n",
      "    [1] \u001b[0m\u001b[1mthrow_api_error\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mres\u001b[39m::\u001b[0mCUDA.cudaError_enum\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mCUDA\u001b[39m \u001b[90m~/.julia/packages/CUDA/k52QH/lib/cudadrv/\u001b[39m\u001b[90;4merror.jl:88\u001b[0m\n",
      "    [2] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/lib/cudadrv/\u001b[39m\u001b[90;4merror.jl:96\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "    [3] \u001b[0m\u001b[1mcuDeviceGetAttribute\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/lib/utils/\u001b[39m\u001b[90;4mcall.jl:26\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "    [4] \u001b[0m\u001b[1mattribute\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mdev\u001b[39m::\u001b[0mCuDevice, \u001b[90mcode\u001b[39m::\u001b[0mCUDA.CUdevice_attribute_enum\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mCUDA\u001b[39m \u001b[90m~/.julia/packages/CUDA/k52QH/lib/cudadrv/\u001b[39m\u001b[90;4mdevices.jl:83\u001b[0m\n",
      "    [5] top-level scope\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/cudadrv/\u001b[39m\u001b[90;4mpool.jl:2\u001b[0m\n",
      "    [6] \u001b[0m\u001b[1minclude\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mclient.jl:444\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "    [7] \u001b[0m\u001b[1m#9\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/\u001b[39m\u001b[90;4mruntests.jl:79\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "    [8] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/\u001b[39m\u001b[90;4msetup.jl:57\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "    [9] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m/buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/\u001b[39m\u001b[90;4mTest.jl:1151\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [10] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/\u001b[39m\u001b[90;4msetup.jl:57\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [11] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/src/\u001b[39m\u001b[90;4mutilities.jl:28\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [12] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/src/\u001b[39m\u001b[90;4mpool.jl:572\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [13] top-level scope\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/\u001b[39m\u001b[90;4msetup.jl:56\u001b[0m\n",
      "   [14] \u001b[0m\u001b[1meval\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mboot.jl:360\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [15] \u001b[0m\u001b[1mruntests\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mf\u001b[39m::\u001b[0mFunction, \u001b[90mname\u001b[39m::\u001b[0mString, \u001b[90mtime_source\u001b[39m::\u001b[0mSymbol, \u001b[90msnoop\u001b[39m::\u001b[0mNothing\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[36mMain\u001b[39m \u001b[90m~/.julia/packages/CUDA/k52QH/test/\u001b[39m\u001b[90;4msetup.jl:68\u001b[0m\n",
      "   [16] \u001b[0m\u001b[1m(::Distributed.var\"#106#108\"{Distributed.CallMsg{:call_fetch}})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[32mDistributed\u001b[39m \u001b[90m/buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Distributed/src/\u001b[39m\u001b[90;4mprocess_messages.jl:278\u001b[0m\n",
      "   [17] \u001b[0m\u001b[1mrun_work_thunk\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mthunk\u001b[39m::\u001b[0mDistributed.var\"#106#108\"\u001b[90m{Distributed.CallMsg{:call_fetch}}\u001b[39m, \u001b[90mprint_error\u001b[39m::\u001b[0mBool\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[32mDistributed\u001b[39m \u001b[90m/buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Distributed/src/\u001b[39m\u001b[90;4mprocess_messages.jl:63\u001b[0m\n",
      "   [18] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m/buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Distributed/src/\u001b[39m\u001b[90;4mprocess_messages.jl:278\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [19] \u001b[0m\u001b[1m(::Distributed.var\"#105#107\"{Distributed.CallMsg{:call_fetch}, Distributed.MsgHeader, Sockets.TCPSocket})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[32mDistributed\u001b[39m \u001b[90m./\u001b[39m\u001b[90;4mtask.jl:406\u001b[0m\n",
      "  in expression starting at /home/ec2-user/.julia/packages/CUDA/k52QH/test/cudadrv/pool.jl:2\n",
      "Worker 2 failed running test cudnn/rnn:\n",
      "\u001b[91mSome tests did not pass: 30 passed, 0 failed, 1 errored, 0 broken.\u001b[39m\n",
      "\u001b[37mcudnn/rnn: \u001b[39m\u001b[91m\u001b[1mError During Test\u001b[22m\u001b[39m at \u001b[39m\u001b[1m/home/ec2-user/.julia/packages/CUDA/k52QH/test/cudnn/rnn.jl:57\u001b[22m\n",
      "  Got exception outside of a @test\n",
      "  CUDNNError: CUDNN_STATUS_NOT_SUPPORTED (code 9)\n",
      "  Stacktrace:\n",
      "    [1] \u001b[0m\u001b[1mthrow_api_error\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mres\u001b[39m::\u001b[0mCUDA.CUDNN.cudnnStatus_t\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mCUDA.CUDNN\u001b[39m \u001b[90m~/.julia/packages/CUDA/k52QH/lib/cudnn/\u001b[39m\u001b[90;4merror.jl:22\u001b[0m\n",
      "    [2] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/lib/cudnn/\u001b[39m\u001b[90;4merror.jl:39\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "    [3] \u001b[0m\u001b[1mcudnnGetRNNTempSpaceSizes\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mhandle\u001b[39m::\u001b[0mPtr\u001b[90m{Nothing}\u001b[39m, \u001b[90mrnnDesc\u001b[39m::\u001b[0mcudnnRNNDescriptor, \u001b[90mfMode\u001b[39m::\u001b[0mcudnnForwardMode_t, \u001b[90mxDesc\u001b[39m::\u001b[0mCUDA.CUDNN.cudnnRNNDataDescriptor, \u001b[90mworkSpaceSize\u001b[39m::\u001b[0mVector\u001b[90m{UInt64}\u001b[39m, \u001b[90mreserveSpaceSize\u001b[39m::\u001b[0mVector\u001b[90m{UInt64}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mCUDA.CUDNN\u001b[39m \u001b[90m~/.julia/packages/CUDA/k52QH/lib/utils/\u001b[39m\u001b[90;4mcall.jl:26\u001b[0m\n",
      "    [4] \u001b[0m\u001b[1mcudnnRNNTempSpaceSizes\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mrnnDesc\u001b[39m::\u001b[0mcudnnRNNDescriptor, \u001b[90mfwdMode\u001b[39m::\u001b[0mcudnnForwardMode_t, \u001b[90mxDesc\u001b[39m::\u001b[0mCUDA.CUDNN.cudnnRNNDataDescriptor\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mCUDA.CUDNN\u001b[39m \u001b[90m~/.julia/packages/CUDA/k52QH/lib/cudnn/\u001b[39m\u001b[90;4mrnn.jl:201\u001b[0m\n",
      "    [5] \u001b[0m\u001b[1mcudnnRNNForwardAD\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mw\u001b[39m::\u001b[0mCuArray\u001b[90m{Float32, 1}\u001b[39m, \u001b[90mx\u001b[39m::\u001b[0mCuArray\u001b[90m{Float32, 3}\u001b[39m, \u001b[90mhx\u001b[39m::\u001b[0mNothing, \u001b[90mcx\u001b[39m::\u001b[0mNothing; \u001b[90mrnnDesc\u001b[39m::\u001b[0mcudnnRNNDescriptor, \u001b[90mfwdMode\u001b[39m::\u001b[0mcudnnForwardMode_t, \u001b[90mdevSeqLengths\u001b[39m::\u001b[0mCuArray\u001b[90m{Int32, 1}\u001b[39m, \u001b[90mxDesc\u001b[39m::\u001b[0mCUDA.CUDNN.cudnnRNNDataDescriptor, \u001b[90myDesc\u001b[39m::\u001b[0mCUDA.CUDNN.cudnnRNNDataDescriptor, \u001b[90my\u001b[39m::\u001b[0mCuArray\u001b[90m{Float32, 3}\u001b[39m, \u001b[90mhDesc\u001b[39m::\u001b[0mcudnnTensorDescriptor, \u001b[90mhy\u001b[39m::\u001b[0mNothing, \u001b[90mcDesc\u001b[39m::\u001b[0mcudnnTensorDescriptor, \u001b[90mcy\u001b[39m::\u001b[0mNothing, \u001b[90mworkspace\u001b[39m::\u001b[0mNothing, \u001b[90mreserveSpace\u001b[39m::\u001b[0mNothing, \u001b[90mdw\u001b[39m::\u001b[0mBase.RefValue\u001b[90m{Any}\u001b[39m, \u001b[90mdx\u001b[39m::\u001b[0mBase.RefValue\u001b[90m{Any}\u001b[39m, \u001b[90mdhx\u001b[39m::\u001b[0mBase.RefValue\u001b[90m{Any}\u001b[39m, \u001b[90mdcx\u001b[39m::\u001b[0mBase.RefValue\u001b[90m{Any}\u001b[39m, \u001b[90mdready\u001b[39m::\u001b[0mBase.RefValue\u001b[90m{Bool}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mCUDA.CUDNN\u001b[39m \u001b[90m~/.julia/packages/CUDA/k52QH/lib/cudnn/\u001b[39m\u001b[90;4mrnn.jl:180\u001b[0m\n",
      "    [6] \u001b[0m\u001b[1mcudnnRNNForwardWithDefaults\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mw\u001b[39m::\u001b[0mCuArray\u001b[90m{Float32, 1}\u001b[39m, \u001b[90mx\u001b[39m::\u001b[0mCuArray\u001b[90m{Float32, 3}\u001b[39m; \u001b[90mhx\u001b[39m::\u001b[0mNothing, \u001b[90mcx\u001b[39m::\u001b[0mNothing, \u001b[90my\u001b[39m::\u001b[0mNothing, \u001b[90mhy\u001b[39m::\u001b[0mNothing, \u001b[90mcy\u001b[39m::\u001b[0mNothing, \u001b[90malgo\u001b[39m::\u001b[0mcudnnRNNAlgo_t, \u001b[90mcellMode\u001b[39m::\u001b[0mcudnnRNNMode_t, \u001b[90mbiasMode\u001b[39m::\u001b[0mcudnnRNNBiasMode_t, \u001b[90mdirMode\u001b[39m::\u001b[0mcudnnDirectionMode_t, \u001b[90minputMode\u001b[39m::\u001b[0mcudnnRNNInputMode_t, \u001b[90mdataType\u001b[39m::\u001b[0mDataType, \u001b[90mmathPrec\u001b[39m::\u001b[0mDataType, \u001b[90mmathType\u001b[39m::\u001b[0mcudnnMathType_t, \u001b[90minputSize\u001b[39m::\u001b[0mInt64, \u001b[90mhiddenSize\u001b[39m::\u001b[0mInt64, \u001b[90mprojSize\u001b[39m::\u001b[0mInt64, \u001b[90mnumLayers\u001b[39m::\u001b[0mInt64, \u001b[90mdropout\u001b[39m::\u001b[0mInt64, \u001b[90mauxFlags\u001b[39m::\u001b[0mUInt32, \u001b[90mrnnDesc\u001b[39m::\u001b[0mcudnnRNNDescriptor, \u001b[90mlayout\u001b[39m::\u001b[0mcudnnRNNDataLayout_t, \u001b[90mseqLengthArray\u001b[39m::\u001b[0mNothing, \u001b[90mpaddingFill\u001b[39m::\u001b[0mPtr\u001b[90m{Nothing}\u001b[39m, \u001b[90mfwdMode\u001b[39m::\u001b[0mcudnnForwardMode_t, \u001b[90mdevSeqLengths\u001b[39m::\u001b[0mNothing, \u001b[90mreserveSpace\u001b[39m::\u001b[0mNothing, \u001b[90mworkspace\u001b[39m::\u001b[0mNothing, \u001b[90mdw\u001b[39m::\u001b[0mBase.RefValue\u001b[90m{Any}\u001b[39m, \u001b[90mdx\u001b[39m::\u001b[0mBase.RefValue\u001b[90m{Any}\u001b[39m, \u001b[90mdhx\u001b[39m::\u001b[0mBase.RefValue\u001b[90m{Any}\u001b[39m, \u001b[90mdcx\u001b[39m::\u001b[0mBase.RefValue\u001b[90m{Any}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mCUDA.CUDNN\u001b[39m \u001b[90m~/.julia/packages/CUDA/k52QH/lib/cudnn/\u001b[39m\u001b[90;4mrnn.jl:170\u001b[0m\n",
      "    [7] \u001b[0m\u001b[1m#cudnnRNNForward#660\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/lib/cudnn/\u001b[39m\u001b[90;4mrnn.jl:41\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "    [8] \u001b[0m\u001b[1m(::var\"#rnntest#59\"{var\"#rnntest#58#60\"{CuArray{Float32, 3}, CuArray{Float32, 1}}, CuArray{Float32, 3}, Int64})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m; \u001b[90mhx\u001b[39m::\u001b[0mNothing, \u001b[90mcx\u001b[39m::\u001b[0mNothing, \u001b[90mhy\u001b[39m::\u001b[0mNothing, \u001b[90mcy\u001b[39m::\u001b[0mNothing, \u001b[90mlayout\u001b[39m::\u001b[0mcudnnRNNDataLayout_t, \u001b[90mseqLengthArray\u001b[39m::\u001b[0mNothing, \u001b[90mfwdMode\u001b[39m::\u001b[0mcudnnForwardMode_t, \u001b[90mhiddenSize\u001b[39m::\u001b[0mInt64, \u001b[90malgo\u001b[39m::\u001b[0mcudnnRNNAlgo_t, \u001b[90mcellMode\u001b[39m::\u001b[0mcudnnRNNMode_t, \u001b[90mbiasMode\u001b[39m::\u001b[0mcudnnRNNBiasMode_t, \u001b[90mdirMode\u001b[39m::\u001b[0mcudnnDirectionMode_t, \u001b[90minputMode\u001b[39m::\u001b[0mcudnnRNNInputMode_t, \u001b[90mmathPrec\u001b[39m::\u001b[0mDataType, \u001b[90mmathType\u001b[39m::\u001b[0mcudnnMathType_t, \u001b[90minputSize\u001b[39m::\u001b[0mInt64, \u001b[90mprojSize\u001b[39m::\u001b[0mInt64, \u001b[90mnumLayers\u001b[39m::\u001b[0mInt64, \u001b[90mdropout\u001b[39m::\u001b[0mInt64, \u001b[90mauxFlags\u001b[39m::\u001b[0mUInt32\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[36mMain\u001b[39m \u001b[90m~/.julia/packages/CUDA/k52QH/test/cudnn/\u001b[39m\u001b[90;4mrnn.jl:92\u001b[0m\n",
      "    [9] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/cudnn/\u001b[39m\u001b[90;4mrnn.jl:122\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [10] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m/buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/\u001b[39m\u001b[90;4mTest.jl:1151\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [11] top-level scope\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/cudnn/\u001b[39m\u001b[90;4mrnn.jl:58\u001b[0m\n",
      "   [12] \u001b[0m\u001b[1minclude\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mclient.jl:444\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [13] \u001b[0m\u001b[1m#9\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/\u001b[39m\u001b[90;4mruntests.jl:79\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [14] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/\u001b[39m\u001b[90;4msetup.jl:57\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [15] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m/buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/\u001b[39m\u001b[90;4mTest.jl:1151\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [16] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/\u001b[39m\u001b[90;4msetup.jl:57\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [17] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/src/\u001b[39m\u001b[90;4mutilities.jl:28\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [18] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/src/\u001b[39m\u001b[90;4mpool.jl:572\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [19] top-level scope\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/\u001b[39m\u001b[90;4msetup.jl:56\u001b[0m\n",
      "   [20] \u001b[0m\u001b[1meval\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mboot.jl:360\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [21] \u001b[0m\u001b[1mruntests\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mf\u001b[39m::\u001b[0mFunction, \u001b[90mname\u001b[39m::\u001b[0mString, \u001b[90mtime_source\u001b[39m::\u001b[0mSymbol, \u001b[90msnoop\u001b[39m::\u001b[0mNothing\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[36mMain\u001b[39m \u001b[90m~/.julia/packages/CUDA/k52QH/test/\u001b[39m\u001b[90;4msetup.jl:68\u001b[0m\n",
      "   [22] \u001b[0m\u001b[1m(::Distributed.var\"#106#108\"{Distributed.CallMsg{:call_fetch}})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[32mDistributed\u001b[39m \u001b[90m/buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Distributed/src/\u001b[39m\u001b[90;4mprocess_messages.jl:278\u001b[0m\n",
      "   [23] \u001b[0m\u001b[1mrun_work_thunk\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mthunk\u001b[39m::\u001b[0mDistributed.var\"#106#108\"\u001b[90m{Distributed.CallMsg{:call_fetch}}\u001b[39m, \u001b[90mprint_error\u001b[39m::\u001b[0mBool\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[32mDistributed\u001b[39m \u001b[90m/buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Distributed/src/\u001b[39m\u001b[90;4mprocess_messages.jl:63\u001b[0m\n",
      "   [24] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m/buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Distributed/src/\u001b[39m\u001b[90;4mprocess_messages.jl:278\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [25] \u001b[0m\u001b[1m(::Distributed.var\"#105#107\"{Distributed.CallMsg{:call_fetch}, Distributed.MsgHeader, Sockets.TCPSocket})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[32mDistributed\u001b[39m \u001b[90m./\u001b[39m\u001b[90;4mtask.jl:406\u001b[0m\n",
      "\n",
      "\u001b[37m\u001b[1mTest Summary:                         | \u001b[22m\u001b[39m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[91m\u001b[1mError  \u001b[22m\u001b[39m\u001b[33m\u001b[1mBroken  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal\u001b[22m\u001b[39m\n",
      "  Overall                             | \u001b[32m9400  \u001b[39m\u001b[91m    2  \u001b[39m\u001b[33m     5  \u001b[39m\u001b[36m 9407\u001b[39m\n",
      "    apiutils                          | \u001b[32m   9  \u001b[39m               \u001b[36m    9\u001b[39m\n",
      "    nvtx                              |                      \u001b[36mNo tests\u001b[39m\n",
      "    nvml                              | \u001b[32m   7  \u001b[39m               \u001b[36m    7\u001b[39m\n",
      "    curand                            | \u001b[32m   1  \u001b[39m               \u001b[36m    1\u001b[39m\n",
      "    pointer                           | \u001b[32m  35  \u001b[39m               \u001b[36m   35\u001b[39m\n",
      "    initialization                    | \u001b[32m  27  \u001b[39m               \u001b[36m   27\u001b[39m\n",
      "    iterator                          | \u001b[32m  30  \u001b[39m               \u001b[36m   30\u001b[39m\n",
      "    utils                             | \u001b[32m   5  \u001b[39m               \u001b[36m    5\u001b[39m\n",
      "    cudadrv/context                   | \u001b[32m  12  \u001b[39m               \u001b[36m   12\u001b[39m\n",
      "    pool                              | \u001b[32m  10  \u001b[39m               \u001b[36m   10\u001b[39m\n",
      "    cudadrv/devices                   | \u001b[32m   6  \u001b[39m               \u001b[36m    6\u001b[39m\n",
      "    cudadrv/errors                    | \u001b[32m   6  \u001b[39m               \u001b[36m    6\u001b[39m\n",
      "    cudadrv/events                    | \u001b[32m   4  \u001b[39m               \u001b[36m    4\u001b[39m\n",
      "    cudadrv/execution                 | \u001b[32m  15  \u001b[39m               \u001b[36m   15\u001b[39m\n",
      "    cudadrv/module                    | \u001b[32m  16  \u001b[39m               \u001b[36m   16\u001b[39m\n",
      "    cudadrv/occupancy                 | \u001b[32m   1  \u001b[39m               \u001b[36m    1\u001b[39m\n",
      "    cudadrv/pool                      |       \u001b[91m    1  \u001b[39m        \u001b[36m    1\u001b[39m\n",
      "    threading                         |                      \u001b[36mNo tests\u001b[39m\n",
      "    cudadrv/memory                    | \u001b[32m  49  \u001b[39m       \u001b[33m     1  \u001b[39m\u001b[36m   50\u001b[39m\n",
      "    cudadrv/profile                   | \u001b[32m   2  \u001b[39m               \u001b[36m    2\u001b[39m\n",
      "    cudadrv/stream                    | \u001b[32m   7  \u001b[39m               \u001b[36m    7\u001b[39m\n",
      "    cudadrv/version                   | \u001b[32m   3  \u001b[39m               \u001b[36m    3\u001b[39m\n",
      "    cudnn/convolution                 |                      \u001b[36mNo tests\u001b[39m\n",
      "    cudnn/activation                  | \u001b[32m  43  \u001b[39m               \u001b[36m   43\u001b[39m\n",
      "    codegen                           | \u001b[32m  10  \u001b[39m               \u001b[36m   10\u001b[39m\n",
      "    cudnn/dropout                     | \u001b[32m   7  \u001b[39m               \u001b[36m    7\u001b[39m\n",
      "    random                            | \u001b[32m 101  \u001b[39m               \u001b[36m  101\u001b[39m\n",
      "    cudnn/optensor                    | \u001b[32m  43  \u001b[39m               \u001b[36m   43\u001b[39m\n",
      "    cudnn/reduce                      | \u001b[32m  51  \u001b[39m               \u001b[36m   51\u001b[39m\n",
      "    cudnn/inplace                     | \u001b[32m  12  \u001b[39m               \u001b[36m   12\u001b[39m\n",
      "    cudnn/softmax                     | \u001b[32m  16  \u001b[39m               \u001b[36m   16\u001b[39m\n",
      "    cudnn/pooling                     | \u001b[32m  48  \u001b[39m               \u001b[36m   48\u001b[39m\n",
      "    cudnn/tensor                      | \u001b[32m  10  \u001b[39m               \u001b[36m   10\u001b[39m\n",
      "    cudnn/multiheadattn               | \u001b[32m  69  \u001b[39m               \u001b[36m   69\u001b[39m\n",
      "    cudnn/rnn                         | \u001b[32m  30  \u001b[39m\u001b[91m    1  \u001b[39m        \u001b[36m   31\u001b[39m\n",
      "    cudnn/normalization               | \u001b[32m  32  \u001b[39m               \u001b[36m   32\u001b[39m\n",
      "    broadcast                         | \u001b[32m  21  \u001b[39m               \u001b[36m   21\u001b[39m\n",
      "    cufft                             | \u001b[32m 175  \u001b[39m               \u001b[36m  175\u001b[39m\n",
      "    device/array                      | \u001b[32m  18  \u001b[39m               \u001b[36m   18\u001b[39m\n",
      "    gpuarrays/math                    | \u001b[32m   8  \u001b[39m               \u001b[36m    8\u001b[39m\n",
      "    cusolver/sparse                   | \u001b[32m  84  \u001b[39m               \u001b[36m   84\u001b[39m\n",
      "    gpuarrays/input output            | \u001b[32m   5  \u001b[39m               \u001b[36m    5\u001b[39m\n",
      "    cusolver/multigpu                 | \u001b[32m  30  \u001b[39m               \u001b[36m   30\u001b[39m\n",
      "    device/ldg                        | \u001b[32m  22  \u001b[39m               \u001b[36m   22\u001b[39m\n",
      "    statistics                        | \u001b[32m  32  \u001b[39m               \u001b[36m   32\u001b[39m\n",
      "    cusparse                          | \u001b[32m 822  \u001b[39m               \u001b[36m  822\u001b[39m\n",
      "    gpuarrays/indexing scalar         | \u001b[32m 249  \u001b[39m               \u001b[36m  249\u001b[39m\n",
      "    cusparse/interfaces               | \u001b[32m  84  \u001b[39m               \u001b[36m   84\u001b[39m\n",
      "    gpuarrays/interface               | \u001b[32m   7  \u001b[39m               \u001b[36m    7\u001b[39m\n",
      "    gpuarrays/constructors            | \u001b[32m 335  \u001b[39m               \u001b[36m  335\u001b[39m\n",
      "    gpuarrays/conversions             | \u001b[32m  72  \u001b[39m               \u001b[36m   72\u001b[39m\n",
      "    texture                           | \u001b[32m  38  \u001b[39m       \u001b[33m     4  \u001b[39m\u001b[36m   42\u001b[39m\n",
      "    gpuarrays/value constructors      | \u001b[32m  36  \u001b[39m               \u001b[36m   36\u001b[39m\n",
      "    gpuarrays/iterator constructors   | \u001b[32m  24  \u001b[39m               \u001b[36m   24\u001b[39m\n",
      "    exceptions                        | \u001b[32m  17  \u001b[39m               \u001b[36m   17\u001b[39m\n",
      "    gpuarrays/uniformscaling          | \u001b[32m  56  \u001b[39m               \u001b[36m   56\u001b[39m\n",
      "    gpuarrays/indexing multidimensional | \u001b[32m  34  \u001b[39m               \u001b[36m   34\u001b[39m\n",
      "    examples                          | \u001b[32m   7  \u001b[39m               \u001b[36m    7\u001b[39m\n",
      "    gpuarrays/random                  | \u001b[32m  46  \u001b[39m               \u001b[36m   46\u001b[39m\n",
      "    execution                         | \u001b[32m  74  \u001b[39m               \u001b[36m   74\u001b[39m\n",
      "    gpuarrays/base                    | \u001b[32m  42  \u001b[39m               \u001b[36m   42\u001b[39m\n",
      "    cublas                            | \u001b[32m2225  \u001b[39m               \u001b[36m 2225\u001b[39m\n",
      "    array                             | \u001b[32m 219  \u001b[39m               \u001b[36m  219\u001b[39m\n",
      "    cusolver/dense                    | \u001b[32m1580  \u001b[39m               \u001b[36m 1580\u001b[39m\n",
      "    sorting                           | \u001b[32m 138  \u001b[39m               \u001b[36m  138\u001b[39m\n",
      "    gpuarrays/broadcasting            | \u001b[32m 157  \u001b[39m               \u001b[36m  157\u001b[39m\n",
      "    gpuarrays/linear algebra          | \u001b[32m 389  \u001b[39m               \u001b[36m  389\u001b[39m\n",
      "    gpuarrays/mapreduce essentials    | \u001b[32m 522  \u001b[39m               \u001b[36m  522\u001b[39m\n",
      "    device/intrinsics                 | \u001b[32m 288  \u001b[39m               \u001b[36m  288\u001b[39m\n",
      "    gpuarrays/mapreduce derivatives   | \u001b[32m 827  \u001b[39m               \u001b[36m  827\u001b[39m\n",
      "    \u001b[31;1mFAILURE\u001b[0m\n",
      "\n",
      "Error in testset cudadrv/pool:\n",
      "\u001b[91m\u001b[1mError During Test\u001b[22m\u001b[39m at \u001b[39m\u001b[1m/home/ec2-user/.julia/packages/CUDA/k52QH/test/setup.jl:56\u001b[22m\n",
      "  Got exception outside of a @test\n",
      "  LoadError: CUDA error: invalid argument (code 1, ERROR_INVALID_VALUE)\n",
      "  Stacktrace:\n",
      "    [1] \u001b[0m\u001b[1mthrow_api_error\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mres\u001b[39m::\u001b[0mCUDA.cudaError_enum\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mCUDA\u001b[39m \u001b[90m~/.julia/packages/CUDA/k52QH/lib/cudadrv/\u001b[39m\u001b[90;4merror.jl:88\u001b[0m\n",
      "    [2] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/lib/cudadrv/\u001b[39m\u001b[90;4merror.jl:96\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "    [3] \u001b[0m\u001b[1mcuDeviceGetAttribute\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/lib/utils/\u001b[39m\u001b[90;4mcall.jl:26\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "    [4] \u001b[0m\u001b[1mattribute\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mdev\u001b[39m::\u001b[0mCuDevice, \u001b[90mcode\u001b[39m::\u001b[0mCUDA.CUdevice_attribute_enum\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mCUDA\u001b[39m \u001b[90m~/.julia/packages/CUDA/k52QH/lib/cudadrv/\u001b[39m\u001b[90;4mdevices.jl:83\u001b[0m\n",
      "    [5] top-level scope\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/cudadrv/\u001b[39m\u001b[90;4mpool.jl:2\u001b[0m\n",
      "    [6] \u001b[0m\u001b[1minclude\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mclient.jl:444\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "    [7] \u001b[0m\u001b[1m#9\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/\u001b[39m\u001b[90;4mruntests.jl:79\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "    [8] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/\u001b[39m\u001b[90;4msetup.jl:57\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "    [9] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m/buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/\u001b[39m\u001b[90;4mTest.jl:1151\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [10] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/\u001b[39m\u001b[90;4msetup.jl:57\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [11] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/src/\u001b[39m\u001b[90;4mutilities.jl:28\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [12] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/src/\u001b[39m\u001b[90;4mpool.jl:572\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [13] top-level scope\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/\u001b[39m\u001b[90;4msetup.jl:56\u001b[0m\n",
      "   [14] \u001b[0m\u001b[1meval\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mboot.jl:360\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [15] \u001b[0m\u001b[1mruntests\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mf\u001b[39m::\u001b[0mFunction, \u001b[90mname\u001b[39m::\u001b[0mString, \u001b[90mtime_source\u001b[39m::\u001b[0mSymbol, \u001b[90msnoop\u001b[39m::\u001b[0mNothing\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[36mMain\u001b[39m \u001b[90m~/.julia/packages/CUDA/k52QH/test/\u001b[39m\u001b[90;4msetup.jl:68\u001b[0m\n",
      "   [16] \u001b[0m\u001b[1m(::Distributed.var\"#106#108\"{Distributed.CallMsg{:call_fetch}})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[32mDistributed\u001b[39m \u001b[90m/buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Distributed/src/\u001b[39m\u001b[90;4mprocess_messages.jl:278\u001b[0m\n",
      "   [17] \u001b[0m\u001b[1mrun_work_thunk\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mthunk\u001b[39m::\u001b[0mDistributed.var\"#106#108\"\u001b[90m{Distributed.CallMsg{:call_fetch}}\u001b[39m, \u001b[90mprint_error\u001b[39m::\u001b[0mBool\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[32mDistributed\u001b[39m \u001b[90m/buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Distributed/src/\u001b[39m\u001b[90;4mprocess_messages.jl:63\u001b[0m\n",
      "   [18] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m/buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Distributed/src/\u001b[39m\u001b[90;4mprocess_messages.jl:278\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [19] \u001b[0m\u001b[1m(::Distributed.var\"#105#107\"{Distributed.CallMsg{:call_fetch}, Distributed.MsgHeader, Sockets.TCPSocket})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[32mDistributed\u001b[39m \u001b[90m./\u001b[39m\u001b[90;4mtask.jl:406\u001b[0m\n",
      "  in expression starting at /home/ec2-user/.julia/packages/CUDA/k52QH/test/cudadrv/pool.jl:2\n",
      "Error in testset cudnn/rnn:\n",
      "\u001b[91m\u001b[1mError During Test\u001b[22m\u001b[39m at \u001b[39m\u001b[1m/home/ec2-user/.julia/packages/CUDA/k52QH/test/cudnn/rnn.jl:57\u001b[22m\n",
      "  Got exception outside of a @test\n",
      "  CUDNNError: CUDNN_STATUS_NOT_SUPPORTED (code 9)\n",
      "  Stacktrace:\n",
      "    [1] \u001b[0m\u001b[1mthrow_api_error\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mres\u001b[39m::\u001b[0mCUDA.CUDNN.cudnnStatus_t\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mCUDA.CUDNN\u001b[39m \u001b[90m~/.julia/packages/CUDA/k52QH/lib/cudnn/\u001b[39m\u001b[90;4merror.jl:22\u001b[0m\n",
      "    [2] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/lib/cudnn/\u001b[39m\u001b[90;4merror.jl:39\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "    [3] \u001b[0m\u001b[1mcudnnGetRNNTempSpaceSizes\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mhandle\u001b[39m::\u001b[0mPtr\u001b[90m{Nothing}\u001b[39m, \u001b[90mrnnDesc\u001b[39m::\u001b[0mcudnnRNNDescriptor, \u001b[90mfMode\u001b[39m::\u001b[0mcudnnForwardMode_t, \u001b[90mxDesc\u001b[39m::\u001b[0mCUDA.CUDNN.cudnnRNNDataDescriptor, \u001b[90mworkSpaceSize\u001b[39m::\u001b[0mVector\u001b[90m{UInt64}\u001b[39m, \u001b[90mreserveSpaceSize\u001b[39m::\u001b[0mVector\u001b[90m{UInt64}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mCUDA.CUDNN\u001b[39m \u001b[90m~/.julia/packages/CUDA/k52QH/lib/utils/\u001b[39m\u001b[90;4mcall.jl:26\u001b[0m\n",
      "    [4] \u001b[0m\u001b[1mcudnnRNNTempSpaceSizes\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mrnnDesc\u001b[39m::\u001b[0mcudnnRNNDescriptor, \u001b[90mfwdMode\u001b[39m::\u001b[0mcudnnForwardMode_t, \u001b[90mxDesc\u001b[39m::\u001b[0mCUDA.CUDNN.cudnnRNNDataDescriptor\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mCUDA.CUDNN\u001b[39m \u001b[90m~/.julia/packages/CUDA/k52QH/lib/cudnn/\u001b[39m\u001b[90;4mrnn.jl:201\u001b[0m\n",
      "    [5] \u001b[0m\u001b[1mcudnnRNNForwardAD\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mw\u001b[39m::\u001b[0mCuArray\u001b[90m{Float32, 1}\u001b[39m, \u001b[90mx\u001b[39m::\u001b[0mCuArray\u001b[90m{Float32, 3}\u001b[39m, \u001b[90mhx\u001b[39m::\u001b[0mNothing, \u001b[90mcx\u001b[39m::\u001b[0mNothing; \u001b[90mrnnDesc\u001b[39m::\u001b[0mcudnnRNNDescriptor, \u001b[90mfwdMode\u001b[39m::\u001b[0mcudnnForwardMode_t, \u001b[90mdevSeqLengths\u001b[39m::\u001b[0mCuArray\u001b[90m{Int32, 1}\u001b[39m, \u001b[90mxDesc\u001b[39m::\u001b[0mCUDA.CUDNN.cudnnRNNDataDescriptor, \u001b[90myDesc\u001b[39m::\u001b[0mCUDA.CUDNN.cudnnRNNDataDescriptor, \u001b[90my\u001b[39m::\u001b[0mCuArray\u001b[90m{Float32, 3}\u001b[39m, \u001b[90mhDesc\u001b[39m::\u001b[0mcudnnTensorDescriptor, \u001b[90mhy\u001b[39m::\u001b[0mNothing, \u001b[90mcDesc\u001b[39m::\u001b[0mcudnnTensorDescriptor, \u001b[90mcy\u001b[39m::\u001b[0mNothing, \u001b[90mworkspace\u001b[39m::\u001b[0mNothing, \u001b[90mreserveSpace\u001b[39m::\u001b[0mNothing, \u001b[90mdw\u001b[39m::\u001b[0mBase.RefValue\u001b[90m{Any}\u001b[39m, \u001b[90mdx\u001b[39m::\u001b[0mBase.RefValue\u001b[90m{Any}\u001b[39m, \u001b[90mdhx\u001b[39m::\u001b[0mBase.RefValue\u001b[90m{Any}\u001b[39m, \u001b[90mdcx\u001b[39m::\u001b[0mBase.RefValue\u001b[90m{Any}\u001b[39m, \u001b[90mdready\u001b[39m::\u001b[0mBase.RefValue\u001b[90m{Bool}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mCUDA.CUDNN\u001b[39m \u001b[90m~/.julia/packages/CUDA/k52QH/lib/cudnn/\u001b[39m\u001b[90;4mrnn.jl:180\u001b[0m\n",
      "    [6] \u001b[0m\u001b[1mcudnnRNNForwardWithDefaults\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mw\u001b[39m::\u001b[0mCuArray\u001b[90m{Float32, 1}\u001b[39m, \u001b[90mx\u001b[39m::\u001b[0mCuArray\u001b[90m{Float32, 3}\u001b[39m; \u001b[90mhx\u001b[39m::\u001b[0mNothing, \u001b[90mcx\u001b[39m::\u001b[0mNothing, \u001b[90my\u001b[39m::\u001b[0mNothing, \u001b[90mhy\u001b[39m::\u001b[0mNothing, \u001b[90mcy\u001b[39m::\u001b[0mNothing, \u001b[90malgo\u001b[39m::\u001b[0mcudnnRNNAlgo_t, \u001b[90mcellMode\u001b[39m::\u001b[0mcudnnRNNMode_t, \u001b[90mbiasMode\u001b[39m::\u001b[0mcudnnRNNBiasMode_t, \u001b[90mdirMode\u001b[39m::\u001b[0mcudnnDirectionMode_t, \u001b[90minputMode\u001b[39m::\u001b[0mcudnnRNNInputMode_t, \u001b[90mdataType\u001b[39m::\u001b[0mDataType, \u001b[90mmathPrec\u001b[39m::\u001b[0mDataType, \u001b[90mmathType\u001b[39m::\u001b[0mcudnnMathType_t, \u001b[90minputSize\u001b[39m::\u001b[0mInt64, \u001b[90mhiddenSize\u001b[39m::\u001b[0mInt64, \u001b[90mprojSize\u001b[39m::\u001b[0mInt64, \u001b[90mnumLayers\u001b[39m::\u001b[0mInt64, \u001b[90mdropout\u001b[39m::\u001b[0mInt64, \u001b[90mauxFlags\u001b[39m::\u001b[0mUInt32, \u001b[90mrnnDesc\u001b[39m::\u001b[0mcudnnRNNDescriptor, \u001b[90mlayout\u001b[39m::\u001b[0mcudnnRNNDataLayout_t, \u001b[90mseqLengthArray\u001b[39m::\u001b[0mNothing, \u001b[90mpaddingFill\u001b[39m::\u001b[0mPtr\u001b[90m{Nothing}\u001b[39m, \u001b[90mfwdMode\u001b[39m::\u001b[0mcudnnForwardMode_t, \u001b[90mdevSeqLengths\u001b[39m::\u001b[0mNothing, \u001b[90mreserveSpace\u001b[39m::\u001b[0mNothing, \u001b[90mworkspace\u001b[39m::\u001b[0mNothing, \u001b[90mdw\u001b[39m::\u001b[0mBase.RefValue\u001b[90m{Any}\u001b[39m, \u001b[90mdx\u001b[39m::\u001b[0mBase.RefValue\u001b[90m{Any}\u001b[39m, \u001b[90mdhx\u001b[39m::\u001b[0mBase.RefValue\u001b[90m{Any}\u001b[39m, \u001b[90mdcx\u001b[39m::\u001b[0mBase.RefValue\u001b[90m{Any}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[35mCUDA.CUDNN\u001b[39m \u001b[90m~/.julia/packages/CUDA/k52QH/lib/cudnn/\u001b[39m\u001b[90;4mrnn.jl:170\u001b[0m\n",
      "    [7] \u001b[0m\u001b[1m#cudnnRNNForward#660\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/lib/cudnn/\u001b[39m\u001b[90;4mrnn.jl:41\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "    [8] \u001b[0m\u001b[1m(::var\"#rnntest#59\"{var\"#rnntest#58#60\"{CuArray{Float32, 3}, CuArray{Float32, 1}}, CuArray{Float32, 3}, Int64})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m; \u001b[90mhx\u001b[39m::\u001b[0mNothing, \u001b[90mcx\u001b[39m::\u001b[0mNothing, \u001b[90mhy\u001b[39m::\u001b[0mNothing, \u001b[90mcy\u001b[39m::\u001b[0mNothing, \u001b[90mlayout\u001b[39m::\u001b[0mcudnnRNNDataLayout_t, \u001b[90mseqLengthArray\u001b[39m::\u001b[0mNothing, \u001b[90mfwdMode\u001b[39m::\u001b[0mcudnnForwardMode_t, \u001b[90mhiddenSize\u001b[39m::\u001b[0mInt64, \u001b[90malgo\u001b[39m::\u001b[0mcudnnRNNAlgo_t, \u001b[90mcellMode\u001b[39m::\u001b[0mcudnnRNNMode_t, \u001b[90mbiasMode\u001b[39m::\u001b[0mcudnnRNNBiasMode_t, \u001b[90mdirMode\u001b[39m::\u001b[0mcudnnDirectionMode_t, \u001b[90minputMode\u001b[39m::\u001b[0mcudnnRNNInputMode_t, \u001b[90mmathPrec\u001b[39m::\u001b[0mDataType, \u001b[90mmathType\u001b[39m::\u001b[0mcudnnMathType_t, \u001b[90minputSize\u001b[39m::\u001b[0mInt64, \u001b[90mprojSize\u001b[39m::\u001b[0mInt64, \u001b[90mnumLayers\u001b[39m::\u001b[0mInt64, \u001b[90mdropout\u001b[39m::\u001b[0mInt64, \u001b[90mauxFlags\u001b[39m::\u001b[0mUInt32\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[36mMain\u001b[39m \u001b[90m~/.julia/packages/CUDA/k52QH/test/cudnn/\u001b[39m\u001b[90;4mrnn.jl:92\u001b[0m\n",
      "    [9] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/cudnn/\u001b[39m\u001b[90;4mrnn.jl:122\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [10] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m/buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/\u001b[39m\u001b[90;4mTest.jl:1151\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [11] top-level scope\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/cudnn/\u001b[39m\u001b[90;4mrnn.jl:58\u001b[0m\n",
      "   [12] \u001b[0m\u001b[1minclude\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mclient.jl:444\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [13] \u001b[0m\u001b[1m#9\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/\u001b[39m\u001b[90;4mruntests.jl:79\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [14] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/\u001b[39m\u001b[90;4msetup.jl:57\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [15] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m/buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Test/src/\u001b[39m\u001b[90;4mTest.jl:1151\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [16] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/\u001b[39m\u001b[90;4msetup.jl:57\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [17] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/src/\u001b[39m\u001b[90;4mutilities.jl:28\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [18] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/src/\u001b[39m\u001b[90;4mpool.jl:572\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [19] top-level scope\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m~/.julia/packages/CUDA/k52QH/test/\u001b[39m\u001b[90;4msetup.jl:56\u001b[0m\n",
      "   [20] \u001b[0m\u001b[1meval\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mboot.jl:360\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [21] \u001b[0m\u001b[1mruntests\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mf\u001b[39m::\u001b[0mFunction, \u001b[90mname\u001b[39m::\u001b[0mString, \u001b[90mtime_source\u001b[39m::\u001b[0mSymbol, \u001b[90msnoop\u001b[39m::\u001b[0mNothing\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[36mMain\u001b[39m \u001b[90m~/.julia/packages/CUDA/k52QH/test/\u001b[39m\u001b[90;4msetup.jl:68\u001b[0m\n",
      "   [22] \u001b[0m\u001b[1m(::Distributed.var\"#106#108\"{Distributed.CallMsg{:call_fetch}})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[32mDistributed\u001b[39m \u001b[90m/buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Distributed/src/\u001b[39m\u001b[90;4mprocess_messages.jl:278\u001b[0m\n",
      "   [23] \u001b[0m\u001b[1mrun_work_thunk\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mthunk\u001b[39m::\u001b[0mDistributed.var\"#106#108\"\u001b[90m{Distributed.CallMsg{:call_fetch}}\u001b[39m, \u001b[90mprint_error\u001b[39m::\u001b[0mBool\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[32mDistributed\u001b[39m \u001b[90m/buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Distributed/src/\u001b[39m\u001b[90;4mprocess_messages.jl:63\u001b[0m\n",
      "   [24] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[90m/buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Distributed/src/\u001b[39m\u001b[90;4mprocess_messages.jl:278\u001b[0m\u001b[90m [inlined]\u001b[39m\n",
      "   [25] \u001b[0m\u001b[1m(::Distributed.var\"#105#107\"{Distributed.CallMsg{:call_fetch}, Distributed.MsgHeader, Sockets.TCPSocket})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n",
      "  \u001b[90m    @ \u001b[39m\u001b[32mDistributed\u001b[39m \u001b[90m./\u001b[39m\u001b[90;4mtask.jl:406\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[91m\u001b[1mERROR: \u001b[22m\u001b[39mLoadError: \u001b[91mTest run finished with errors\u001b[39m\n",
      "in expression starting at /home/ec2-user/.julia/packages/CUDA/k52QH/test/runtests.jl:490\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "Package CUDA errored during testing",
     "output_type": "error",
     "traceback": [
      "Package CUDA errored during testing",
      "",
      "Stacktrace:",
      "  [1] pkgerror(msg::String)",
      "    @ Pkg.Types /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/Types.jl:55",
      "  [2] test(ctx::Pkg.Types.Context, pkgs::Vector{Pkg.Types.PackageSpec}; coverage::Bool, julia_args::Cmd, test_args::Cmd, test_fn::Nothing)",
      "    @ Pkg.Operations /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/Operations.jl:1687",
      "  [3] test(ctx::Pkg.Types.Context, pkgs::Vector{Pkg.Types.PackageSpec}; coverage::Bool, test_fn::Nothing, julia_args::Cmd, test_args::Cmd, kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ Pkg.API /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:336",
      "  [4] test(ctx::Pkg.Types.Context, pkgs::Vector{Pkg.Types.PackageSpec})",
      "    @ Pkg.API /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:323",
      "  [5] #test#62",
      "    @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:73 [inlined]",
      "  [6] test",
      "    @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:72 [inlined]",
      "  [7] #test#61",
      "    @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:70 [inlined]",
      "  [8] test",
      "    @ /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:70 [inlined]",
      "  [9] test(pkg::String; kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ Pkg.API /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:69",
      " [10] test(pkg::String)",
      "    @ Pkg.API /buildworker/worker/package_linux64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:69",
      " [11] top-level scope",
      "    @ In[2]:1",
      " [12] eval",
      "    @ ./boot.jl:360 [inlined]",
      " [13] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1094"
     ]
    }
   ],
   "source": [
    "Pkg.test(\"CUDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3faaa8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia-16-threads 1.6.0",
   "language": "julia",
   "name": "julia-16-threads-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
